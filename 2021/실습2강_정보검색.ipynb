{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ì‹¤ìŠµ2ê°•_ì •ë³´ê²€ìƒ‰.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Seo/BDC103_IR/blob/main/%EC%8B%A4%EC%8A%B52%EA%B0%95_%EC%A0%95%EB%B3%B4%EA%B2%80%EC%83%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx6X7EIQKF3e"
      },
      "source": [
        "## **ì‹¤ìŠµ 2.**\n",
        "\n",
        "ë³¸ ì‹¤ìŠµ ìë£ŒëŠ” **BDC103(00) ë¹…ë°ì´í„°ì™€ì •ë³´ê²€ìƒ‰ ê°•ì˜ ì‹¤ìŠµ**ì„ ìœ„í•´ **ê³ ë ¤ëŒ€í•™êµ ìì—°ì–´ì²˜ë¦¬ì—°êµ¬ì‹¤ (NLP & AI Lab)**ì—ì„œ ì œì‘í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "â˜ ï¸ ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ë°°í¬ë¥¼ ê¸ˆì§€í•©ë‹ˆë‹¤. â˜ ï¸\n",
        "\n",
        "```\n",
        "version 1.0 (2021.03.04)\n",
        "created by: ì„œì¬í˜•, ì„í¬ì„ (ê³ ë ¤ëŒ€í•™êµ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬ì‹¤)\n",
        "email: wolhalang@gmail.com\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaK6IVShmEa3"
      },
      "source": [
        "## **ì •ë³´ ê²€ìƒ‰ì˜ ë°©ë²• ğŸ˜Š**\n",
        "\n",
        "ë¬¸ì„œ ê²€ìƒ‰ì—ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì¸ Boolean (0,1)ë¡œ êµ¬ì„±.\n",
        "\n",
        "ê¸°ì¡´ì˜ ë¬¸ì„œ ë‹¨ìœ„ í–‰ë ¬ Document Term Matrix (DTM)ì€ ë‹¨ì–´ì˜ ì¶œí˜„ ì—¬ë¶€ì— ë”°ë¼ì„œ 0ê³¼ 1ë¡œ í‘œí˜„.\n",
        "\n",
        "**ê·¸ëŸ¬ë‚˜, DTMì€ ì–´ë–¤ ë¬¸ì„œê°€ ë” ì¤‘ìš”í•œì§€, ë”ìš± ì¼ì¹˜í–ëŠ”ì§€ ìˆœìœ„ë¥¼ ë¶€ì—¬í•  ìˆ˜ ì—†ìŒ.**\n",
        "\n",
        "ë”°ë¼ì„œ, **ë‹¨ì–´ ë¹ˆë„ìˆ˜ Term-Frequency (TF)ì™€ ë¬¸ì„œ ì—­ë¹ˆë„ìˆ˜ Inverse Document Frquency (IDF)** ë¥¼ í†µí•´ ë¬¸ì„œ ì²˜ë¦¬.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFqH3RJmqlqu"
      },
      "source": [
        "### **TF-IDF** \n",
        "\n",
        "> **Term Frequency (TF)**ëŠ” ë‹¨ì–´ê°€ ë¬¸ì„œì— ë‚˜íƒ€ë‚œ íšŸìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, í•´ë‹¹ ë‹¨ì–´ê°€ ë¬¸ì„œì— ìì£¼ ë“±ì¥í•  ìˆ˜ë¡ ìˆ˜ì¹˜ê°€ ì¦ê°€í•˜ì—¬ ë†’ì€ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "> **Document Frequency (DF)**ëŠ” í•´ë‹¹ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, í•´ë‹¹ ë‹¨ì–´ê°€ ë‹¤ì¤‘ ë¬¸ì„œì— ìì£¼ ë“±ì¥í•  ìˆ˜ë¡ ìˆ˜ì¹˜ê°€ ì¦ê°€í•˜ì—¬ ë‚®ì€ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "> **Inverse Document Frequency (IDF)**ëŠ” DFì˜ ì—­ìˆ˜ë¡œ ì „ì²´ ë‹¨ì–´ ìˆ˜ë¥¼ í•´ë‹¹ ë‹¨ì–´ì˜ DFë¡œ ë‚˜ëˆˆ ë’¤ ë¡œê·¸(log)ë¥¼ ì·¨í•œ ê°’ì´ë‹¤. ì¦‰ ê°’ì´ í´ìˆ˜ë¡ ì¤‘ìš”í•´ì§ˆ ìˆ˜ ìˆë„ë¡ ì—­ìˆ˜ë¥¼ ì·¨í•œë‹¤ (TFì™€ì˜ ê³„ì‚° í¸ì˜ì„±ì„ ìœ„í•´)\n",
        "\n",
        "> **TF-IDFëŠ” TFì™€ IDFì˜ ê³±**ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ë‘ ì§€í‘œë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ëŠ” ê°€ì¤‘ì¹˜ ì‚°ì¶œ ë°©ë²•ì´ë‹¤. \n",
        "\n",
        "\n",
        "$$tf(d,t)$$\n",
        "\n",
        "$$idf(d,t) = {log({n\\over 1+df(t)})}$$\n",
        "\n",
        "$$ w_d,_t = tf(d,t) * idf(d,t) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGNFrUrItkpy"
      },
      "source": [
        "### **TF-IDF êµ¬í˜„** ğŸ¤©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Qxmd8atohM"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meBFlkGdtzxP",
        "outputId": "2bf5810e-b941-4386-9d5c-39bb4f2b9b33"
      },
      "source": [
        "# ê³ ë ¤ëŒ€í•™êµ í˜¸ìƒë¹„ë¬¸ì˜ êµ¬ì ˆì„ ì½”í¼ìŠ¤ë¡œ í•´ë´…ì‹œë‹¤.\n",
        "epitaph = [\n",
        "  'ë¯¼ì¡±ì˜ í˜ìœ¼ë¡œ ë¯¼ì¡±ì˜ ê¿ˆì„ ê°€ê¾¸ì–´ì˜¨',\n",
        "  'ë¯¼ì¡±ì˜ ë³´ëŒì°¬ ëŒ€í•™ì´ ìˆì–´',\n",
        "  'ë„ˆ í•­ìƒ ì—¬ê¸°ì— ììœ ì˜ ë¶ˆì„ ë°íˆê³ ',\n",
        "  'ì •ì˜ì˜ ê¸¸ì„ ë‹¬ë¦¬ê³  ì§„ë¦¬ì˜ ìƒ˜ì„ ì§€í‚¤ëŠë‹ˆ',\n",
        "  'ì§€ì¶•ì„ ë°•ì°¨ê³  í¬íš¨í•˜ê±°ë¼ ë„ˆ ë¶ˆíƒ€ëŠ” ì•¼ë§',\n",
        "  'ì Šì€ ì˜ìš•ì˜ ìƒì§•ì•„ ìš°ì£¼ë¥¼ í–¥í•œ ë„ˆì˜ ë¶€ë¥´ì§–ìŒì´',\n",
        "  'ë¯¼ì¡±ì˜ ì†Œë¦¬ë˜ì–´ ë©”ì•„ë¦¬ì¹˜ëŠ” ê³³ì— ë„ˆì˜ ê¸°ê°œ',\n",
        "  'ë„ˆì˜ ì§€ì¡° ë„ˆì˜ ì˜ˆì§€ëŠ”',\n",
        "  'ì¡°êµ­ì˜ ì˜ì›í•œ ê³ ë™ì´ ë˜ë¦¬ë¼'\n",
        "] \n",
        "\n",
        "vocab = []\n",
        "\n",
        "# ê° ë¼ì¸ì„ documentë¡œ ìƒê°í•´ì£¼ì„¸ìš”.\n",
        "for line in epitaph:\n",
        "    for token in line.split():\n",
        "        vocab.append(token)\n",
        "\n",
        "print(vocab)\n",
        "\n",
        "vocab = list(set(vocab))\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ë¯¼ì¡±ì˜', 'í˜ìœ¼ë¡œ', 'ë¯¼ì¡±ì˜', 'ê¿ˆì„', 'ê°€ê¾¸ì–´ì˜¨', 'ë¯¼ì¡±ì˜', 'ë³´ëŒì°¬', 'ëŒ€í•™ì´', 'ìˆì–´', 'ë„ˆ', 'í•­ìƒ', 'ì—¬ê¸°ì—', 'ììœ ì˜', 'ë¶ˆì„', 'ë°íˆê³ ', 'ì •ì˜ì˜', 'ê¸¸ì„', 'ë‹¬ë¦¬ê³ ', 'ì§„ë¦¬ì˜', 'ìƒ˜ì„', 'ì§€í‚¤ëŠë‹ˆ', 'ì§€ì¶•ì„', 'ë°•ì°¨ê³ ', 'í¬íš¨í•˜ê±°ë¼', 'ë„ˆ', 'ë¶ˆíƒ€ëŠ”', 'ì•¼ë§', 'ì Šì€', 'ì˜ìš•ì˜', 'ìƒì§•ì•„', 'ìš°ì£¼ë¥¼', 'í–¥í•œ', 'ë„ˆì˜', 'ë¶€ë¥´ì§–ìŒì´', 'ë¯¼ì¡±ì˜', 'ì†Œë¦¬ë˜ì–´', 'ë©”ì•„ë¦¬ì¹˜ëŠ”', 'ê³³ì—', 'ë„ˆì˜', 'ê¸°ê°œ', 'ë„ˆì˜', 'ì§€ì¡°', 'ë„ˆì˜', 'ì˜ˆì§€ëŠ”', 'ì¡°êµ­ì˜', 'ì˜ì›í•œ', 'ê³ ë™ì´', 'ë˜ë¦¬ë¼']\n",
            "['í¬íš¨í•˜ê±°ë¼', 'ë¶€ë¥´ì§–ìŒì´', 'ë°íˆê³ ', 'ì˜ˆì§€ëŠ”', 'ë¶ˆíƒ€ëŠ”', 'ë˜ë¦¬ë¼', 'ì•¼ë§', 'ì§€ì¡°', 'ë¯¼ì¡±ì˜', 'ì§€ì¶•ì„', 'í•­ìƒ', 'ê³ ë™ì´', 'ë‹¬ë¦¬ê³ ', 'ë„ˆ', 'ë°•ì°¨ê³ ', 'í–¥í•œ', 'ììœ ì˜', 'ê³³ì—', 'ìˆì–´', 'ìƒì§•ì•„', 'ëŒ€í•™ì´', 'ì—¬ê¸°ì—', 'í˜ìœ¼ë¡œ', 'ìƒ˜ì„', 'ì¡°êµ­ì˜', 'ì§€í‚¤ëŠë‹ˆ', 'ê°€ê¾¸ì–´ì˜¨', 'ê¸°ê°œ', 'ê¸¸ì„', 'ì§„ë¦¬ì˜', 'ë©”ì•„ë¦¬ì¹˜ëŠ”', 'ë„ˆì˜', 'ì˜ìš•ì˜', 'ë³´ëŒì°¬', 'ì Šì€', 'ì˜ì›í•œ', 'ìš°ì£¼ë¥¼', 'ì •ì˜ì˜', 'ì†Œë¦¬ë˜ì–´', 'ë¶ˆì„', 'ê¿ˆì„']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIUwkRfCvmiM"
      },
      "source": [
        "# TF, IDF êµ¬í˜„ í•¨ìˆ˜\n",
        "def tf(t,d):\n",
        "    return d.count(t)\n",
        "\n",
        "def idf(t,document):\n",
        "    df_count = 0\n",
        "    for doc in document:\n",
        "        if t in doc:\n",
        "            df_count += 1\n",
        "    return np.log(len(epitaph)/(df_count+1))\n",
        "\n",
        "def tfidf(t,d):\n",
        "    return tf(t,d)*idf(t,epitaph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "6fcB3de7wjB3",
        "outputId": "cf715ad8-1ab7-4f30-aeca-8dadf1715350"
      },
      "source": [
        "# TF-IDF êµ¬í˜„\n",
        "result = []\n",
        "for i in range(len(epitaph)):\n",
        "    result.append([])\n",
        "    d = epitaph[i]\n",
        "    for j in range(len(vocab)):\n",
        "        t = vocab[j]\n",
        "        result[-1].append(tfidf(t,d))\n",
        "\n",
        "tf_idf = pd.DataFrame(result, columns = vocab)\n",
        "tf_idf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>í¬íš¨í•˜ê±°ë¼</th>\n",
              "      <th>ë¶€ë¥´ì§–ìŒì´</th>\n",
              "      <th>ë°íˆê³ </th>\n",
              "      <th>ì˜ˆì§€ëŠ”</th>\n",
              "      <th>ë¶ˆíƒ€ëŠ”</th>\n",
              "      <th>ë˜ë¦¬ë¼</th>\n",
              "      <th>ì•¼ë§</th>\n",
              "      <th>ì§€ì¡°</th>\n",
              "      <th>ë¯¼ì¡±ì˜</th>\n",
              "      <th>ì§€ì¶•ì„</th>\n",
              "      <th>í•­ìƒ</th>\n",
              "      <th>ê³ ë™ì´</th>\n",
              "      <th>ë‹¬ë¦¬ê³ </th>\n",
              "      <th>ë„ˆ</th>\n",
              "      <th>ë°•ì°¨ê³ </th>\n",
              "      <th>í–¥í•œ</th>\n",
              "      <th>ììœ ì˜</th>\n",
              "      <th>ê³³ì—</th>\n",
              "      <th>ìˆì–´</th>\n",
              "      <th>ìƒì§•ì•„</th>\n",
              "      <th>ëŒ€í•™ì´</th>\n",
              "      <th>ì—¬ê¸°ì—</th>\n",
              "      <th>í˜ìœ¼ë¡œ</th>\n",
              "      <th>ìƒ˜ì„</th>\n",
              "      <th>ì¡°êµ­ì˜</th>\n",
              "      <th>ì§€í‚¤ëŠë‹ˆ</th>\n",
              "      <th>ê°€ê¾¸ì–´ì˜¨</th>\n",
              "      <th>ê¸°ê°œ</th>\n",
              "      <th>ê¸¸ì„</th>\n",
              "      <th>ì§„ë¦¬ì˜</th>\n",
              "      <th>ë©”ì•„ë¦¬ì¹˜ëŠ”</th>\n",
              "      <th>ë„ˆì˜</th>\n",
              "      <th>ì˜ìš•ì˜</th>\n",
              "      <th>ë³´ëŒì°¬</th>\n",
              "      <th>ì Šì€</th>\n",
              "      <th>ì˜ì›í•œ</th>\n",
              "      <th>ìš°ì£¼ë¥¼</th>\n",
              "      <th>ì •ì˜ì˜</th>\n",
              "      <th>ì†Œë¦¬ë˜ì–´</th>\n",
              "      <th>ë¶ˆì„</th>\n",
              "      <th>ê¿ˆì„</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.62186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.810930</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.62186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      í¬íš¨í•˜ê±°ë¼     ë¶€ë¥´ì§–ìŒì´       ë°íˆê³   ...      ì†Œë¦¬ë˜ì–´        ë¶ˆì„        ê¿ˆì„\n",
              "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.504077\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2  0.000000  0.000000  1.504077  ...  0.000000  1.504077  0.000000\n",
              "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "4  1.504077  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "5  0.000000  1.504077  0.000000  ...  0.000000  0.000000  0.000000\n",
              "6  0.000000  0.000000  0.000000  ...  1.504077  0.000000  0.000000\n",
              "7  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "8  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[9 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3IbphCzOTu"
      },
      "source": [
        "### **BM25** \n",
        "\n",
        "â€ $f(q_i, D)$: ë¬¸ì„œì— ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œ ê°¯ìˆ˜\n",
        "\n",
        "â $k_1$: ì„ì˜ì˜ ìƒìˆ˜, default 1.2\n",
        "\n",
        "â‚ $b$: ì„ì˜ì˜ ìƒìˆ˜, default 0.75\n",
        "\n",
        "âƒ $|D|$: í˜„ì¬ ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸¸ì´\n",
        "\n",
        "â„ $avgDL$: ì „ì²´ ë¬¸ì„œì˜ í‰ê· ì ì¸ ê¸¸ì´\n",
        "\n",
        "\n",
        "> `score(Document D, Query q)`ì— ëŒ€í•œ BM25 ìˆ˜ì‹\n",
        "\n",
        "$$ \\sum_{i=1}^{n}IDF(q_i)\\cdot\\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{avgDL})} $$\n",
        "\n",
        "\n",
        "> BM25ëŠ” TF-IDFì™€ ê°™ì€ ìŠ¤ì½”ì–´ ê³„ì‚° ë°©ì‹ì´ì§€ë§Œ **ìƒ‰ì¸í™”ëœ í‰ê·  ë¬¸ì„œì˜ ê¸¸ì´**ì™€ **ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê¸¸ì´ë¥¼ ê³ ë ¤**í–ˆë‹¤ê³  ë³´ë©´ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DG_dw8vPzBp"
      },
      "source": [
        "### **BM25 êµ¬í˜„** ğŸ¤©\r\n",
        "\r\n",
        "> pythonì˜ ê²½ìš° `rank-bm25` íŒ¨í‚¤ì§€ë¥¼ í†µí•´ì„œ BM25ë¥¼ ê°„í¸í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n",
        "\r\n",
        "```\r\n",
        "pip install rank_bm25\r\n",
        "```\r\n",
        "\r\n",
        "> íŒ¨í‚¤ì§€ ì°¸ì¡°\r\n",
        "\r\n",
        "https://pypi.org/project/rank-bm25/\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUUeZ66ABhF_",
        "outputId": "182d26ee-8df9-4284-f11c-4aeac75b4588"
      },
      "source": [
        "!pip install rank_bm25"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LLZWkMWECXU"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\r\n",
        "\r\n",
        "## ì‘ì›ê°€ ì œëª©ì„ ì½”í¼ìŠ¤í™”\r\n",
        "songs = [\r\n",
        "  'ë¯¼ì¡±ì˜ ì•„ë¦¬ì•„',\r\n",
        "  'ì Šì€ ê·¸ëŒ€',\r\n",
        "  'ë“¤ì–´ë¼ ë³´ì•„ë¼ ê¸°ì–µí•˜ë¼',\r\n",
        "  'ë±ƒë…¸ë˜',\r\n",
        "  'ì„íƒ‘',\r\n",
        "  'ì—˜ë¦¬ì œë¥¼ ìœ„í•˜ì—¬',\r\n",
        "  'ê³ ë˜ì‚¬ëƒ¥',\r\n",
        "  'ì§€ì•¼ì˜ í•¨ì„±',\r\n",
        "  'ë ˆì´ëª½ë“œ ì„œê³¡'\r\n",
        "]\r\n",
        "\r\n",
        "## ì½”í¼ìŠ¤ì— ëŒ€í•´ì„œ BM25 ì•Œê³ ë¦¬ì¦˜ ì ìš©ì„ ìœ„í•´ í† í°í™”í•˜ì—¬ ìƒ‰ì¸í™”\r\n",
        "tokenized_songs = [song.split(\" \") for song in songs]\r\n",
        "bm25 = BM25Okapi(tokenized_songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaAsk4kmFo_z",
        "outputId": "fc072bda-eddf-48fe-fbd5-a9e2b2d009d1"
      },
      "source": [
        "# ì§ˆì˜ë¥¼ ë§Œë“¤ì–´ë³´ê³  ìƒ‰ì¸í™”ëœ ë¬¸ì„œì— ëŒ€í•œ doc_scoresë¥¼ ë°˜í™˜\r\n",
        "query = \"ë¯¼ì¡±ì˜ ë…¸ë˜\"\r\n",
        "tokenized_query = query.split(\" \")\r\n",
        "doc_scores = bm25.get_scores(tokenized_query)\r\n",
        "print(doc_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.64222585 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PqseGiMGJ-O",
        "outputId": "e12ccc96-4a17-44cc-fe5f-38828016a0bd"
      },
      "source": [
        "# ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê²°ê³¼ ë°˜í™˜\r\n",
        "bm25.get_top_n(tokenized_query, songs, n=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ë¯¼ì¡±ì˜ ì•„ë¦¬ì•„']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjzBPl_lNJZj"
      },
      "source": [
        "<img src = 'https://miro.medium.com/max/1200/1*zvPL19PUTMslrhXPODj_Og.png' width = 300>\r\n",
        "\r\n",
        "ë‹¤ìŒì€... **BM25 ì•Œê³ ë¦¬ì¦˜**ì„ ê¸°ë³¸ ë°©ë²•ë¡ ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” \r\n",
        "\r\n",
        "**ElasticSearch ê²€ìƒ‰ ì—”ì§„**ì„ **Python**ì—ì„œ í™œìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ë°°ìš°ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR5VDJ1HJfZp"
      },
      "source": [
        "## **ElasticSearchë¥¼ í™œìš©í•œ ì •ë³´ ê²€ìƒ‰** ğŸ§\r\n",
        "\r\n",
        "ê³µì‹ í™ˆí˜ì´ì§€ ì°¸ì¡°: https://www.elastic.co/kr/what-is/elasticsearch\r\n",
        "\r\n",
        "#### **ElasticSearchë€?**\r\n",
        "\r\n",
        "> ElasticsearchëŠ” **í…ìŠ¤íŠ¸, ìˆ«ì, ìœ„ì¹˜ ê¸°ë°˜ ì •ë³´, ì •í˜• ë° ë¹„ì •í˜• ë°ì´í„° ë“± ëª¨ë“  ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ìœ„í•œ ë¬´ë£Œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìœ¼ë¡œ ë¶„ì‚°í˜• ë° ê°œë°©í˜•ì„ íŠ¹ì§•**ìœ¼ë¡œ í•©ë‹ˆë‹¤. ElasticsearchëŠ” Apache Luceneì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìœ¼ë©°, Elasticsearch N.V.(í˜„ì¬ ëª…ì¹­ Elastic)ê°€ 2010ë…„ì— ìµœì´ˆë¡œ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ REST API, ë¶„ì‚°í˜• íŠ¹ì§•, ì†ë„, í™•ì¥ì„±ìœ¼ë¡œ ìœ ëª…í•œ ElasticsearchëŠ” ë°ì´í„° ìˆ˜ì§‘, ë³´ê°•, ì €ì¥, ë¶„ì„, ì‹œê°í™”ë¥¼ ìœ„í•œ ë¬´ë£Œ ê°œë°©í˜• ë„êµ¬ ëª¨ìŒì¸ Elastic Stackì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤.\r\n",
        "\r\n",
        "#### **ElasticSearchì—ì„œ ìƒ‰ì¸ì´ë€?**\r\n",
        "\r\n",
        "> Elasticsearch **ì¸ë±ìŠ¤ëŠ” ì„œë¡œ ê´€ë ¨ë˜ì–´ ìˆëŠ” ë¬¸ì„œë“¤ì˜ ëª¨ìŒ**ì…ë‹ˆë‹¤. ElasticsearchëŠ” **JSON ë¬¸ì„œë¡œ ë°ì´í„°ë¥¼ ì €ì¥**í•©ë‹ˆë‹¤. ê° ë¬¸ì„œëŠ” **ì¼ë ¨ì˜ í‚¤**(í•„ë“œë‚˜ ì†ì„±ì˜ ì´ë¦„)ì™€ **ê·¸ì— í•´ë‹¹í•˜ëŠ” ê°’**(ë¬¸ìì—´, ìˆ«ì, ë¶€ìš¸, ë‚ ì§œ, ê°’ì˜ ë°°ì—´, ì§€ë¦¬ì  ìœ„ì¹˜ ë˜ëŠ” ê¸°íƒ€ ë°ì´í„° ìœ í˜•)ì„ ì„œë¡œ ì—°ê²°í•©ë‹ˆë‹¤.\r\n",
        "\r\n",
        "> ElasticsearchëŠ” **ì—­ ì¸ë±ìŠ¤**ë¼ê³  í•˜ëŠ” ë°ì´í„° êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ê²ƒì€ ì•„ì£¼ ë¹ ë¥¸ í’€í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ê²ƒì…ë‹ˆë‹¤. ì—­ ì¸ë±ìŠ¤ëŠ” **ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ëª¨ë“  ê³ ìœ í•œ ë‹¨ì–´ì˜ ëª©ë¡ì„ ë§Œë“¤ê³ , ê° ë‹¨ì–´ê°€ ë°œìƒí•˜ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ì‹ë³„**í•©ë‹ˆë‹¤.\r\n",
        "\r\n",
        "> ìƒ‰ì¸ í”„ë¡œì„¸ìŠ¤ ì¤‘ì—, **ElasticsearchëŠ” ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ì—­ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì—¬ ê±°ì˜ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰** ê°€ëŠ¥í•œ ë°ì´í„°ë¡œ ë§Œë“­ë‹ˆë‹¤. ì¸ë±ìŠ¤ APIë¥¼ ì‚¬ìš©í•´ ìƒ‰ì¸ì´ ì‹œì‘ë˜ë©°, ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” íŠ¹ì •í•œ ì¸ë±ìŠ¤ì—ì„œ JSON ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\r\n",
        "\r\n",
        "#### **ì‚¬ìš©í•˜ëŠ” ì´ìœ ?**\r\n",
        "\r\n",
        "> ElasticsearchëŠ” Luceneì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ê¸° ë•Œë¬¸ì—, **í’€í…ìŠ¤íŠ¸ ê²€ìƒ‰ì— ë›°ì–´ë‚©ë‹ˆë‹¤**. ElasticsearchëŠ” ë˜í•œ ê±°ì˜ **ì‹¤ì‹œê°„ ê²€ìƒ‰ í”Œë«í¼**ì…ë‹ˆë‹¤. ì´ê²ƒì€ ë¬¸ì„œê°€ ìƒ‰ì¸ë  ë•Œë¶€í„° ê²€ìƒ‰ ê°€ëŠ¥í•´ì§ˆ ë•Œê¹Œì§€ì˜ ëŒ€ê¸° ì‹œê°„ì´ ì•„ì£¼ ì§§ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ì´ ëŒ€ê¸° ì‹œê°„ì€ ë³´í†µ 1ì´ˆì…ë‹ˆë‹¤.\r\n",
        "\r\n",
        "> ë¶„ì‚° ì‹œìŠ¤í…œìœ¼ë¡œ **ë³‘ë ¬ì ì¸ ì²˜ë¦¬. ê²€ìƒ‰ ëŒ€ìƒì˜ ì‚¬ì´ì¦ˆê°€ ì•„ì£¼ í¬ë”ë¼ë„ ë¶„ì„ ë° ì²˜ë¦¬ê°€ ê°€ëŠ¥**í•©ë‹ˆë‹¤.\r\n",
        "\r\n",
        "**--> ë¹…ë°ì´í„° ê²€ìƒ‰ì— ì í•©í•œ ê²€ìƒ‰ ì—”ì§„**\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJ97wRkZIZr"
      },
      "source": [
        "### **êµ¬ê¸€ ì½”ë©ê³¼ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ë™í•˜ê¸°** ğŸ˜\n",
        "\n",
        "êµ¬ê¸€ ì½”ë© í™˜ê²½ì—ì„œëŠ” ë¡œê·¸ì¸ë˜ì–´ ìˆëŠ” êµ¬ê¸€ ê³„ì •ê³¼ ì—°ê²°ë˜ì–´ ìˆëŠ” 'êµ¬ê¸€ ë“œë¼ì´ë¸Œ'ì™€ì˜ ì—°ë™ì„ ì§€ì›í•©ë‹ˆë‹¤.  \n",
        "ì—°ë™í•˜ëŠ” ê²½ìš°, ìì‹ ì˜ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë¡œì»¬ í™˜ê²½ì²˜ëŸ¼ í™œìš©ì´ ê°€ëŠ¥í•˜ë©°, ì›í•˜ëŠ” íŒŒì¼ì´ë‚˜ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZGi15R3ZBNE"
      },
      "source": [
        "### Google Drive íŒ¨í‚¤ì§€ì™€ os ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "from google.colab import drive\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QESTJMWZJgs",
        "outputId": "0f942317-7598-4366-f277-e459ed9983dd"
      },
      "source": [
        "### ë³¸ì¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œì˜ ìµœì´ˆ ê²½ë¡œë¥¼ ì„¤ì •í•˜ê¸°\n",
        "\n",
        "# ëŒ€ë¶€ë¶„ì˜ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ìµœì´ˆ ê²½ë¡œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. \n",
        "# ì˜ˆì™¸ ë°œìƒ ì‹œ ë³¸ì¸ì˜ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì ‘ì†í•˜ì—¬ content í´ë”ë‚˜ gdrive í´ë”ê°€ ì–´ë–¤ ìœ„ì¹˜ì— ìˆìœ¼ë©°, \n",
        "# ìì‹ ì´ ì—°ê²°í•˜ë ¤ëŠ” í´ë”ê¹Œì§€ì˜ ê²½ë¡œê°€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.  \n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BGTF2zT6ZMp6",
        "outputId": "c15171c4-7e45-4a39-88f5-90c7d77c0b12"
      },
      "source": [
        "### ìì‹ ì˜ í˜„ì¬ ê²½ë¡œë¥¼ íŒŒì•…í•˜ê¸°\n",
        "\n",
        "os.getcwd() ## ì•„ë§ˆë„ /contentë¡œ ë‚˜ì˜¬ ê²ƒ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRCwpRm1ije"
      },
      "source": [
        "### **ì—˜ë¼ìŠ¤í‹± ì„œë²„ ì„¤ì¹˜ ë° êµ¬ë™í•˜ê¸°** ğŸ˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_DmUR_krAJN"
      },
      "source": [
        "### êµ¬ê¸€ í´ë¼ìš°ë“œ ì»´í“¨í„°ì— elastic server ì„œë²„ ì„¤ì¹˜ë¥¼ ìœ„í•œ í´ë” ìƒì„± \n",
        "!sudo mkdir /content/elasticsearch\n",
        "### ì ‘ê·¼ ê¶Œí•œ ìˆ˜ì •\n",
        "!chmod 755 -R elasticsearch\n",
        "### í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
        "os.chdir('/content/elasticsearch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CcNnHCosV4Y"
      },
      "source": [
        "### ë¦¬ëˆ…ìŠ¤ìš© ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ ì„œë²„ ì„¤ì¹˜ë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4UVAmFWsd_Z"
      },
      "source": [
        "### ìœ„ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì€ ì••ì¶• íŒŒì¼ì„ í•´ì œ\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfUtZwAHskoH"
      },
      "source": [
        "### ì½”ë© ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ì„œë²„ êµ¬ë™ì„ ìœ„í•´ì„œ PPID 1ì˜ ë°±ê·¸ë¼ìš´ë“œ ë°ëª¬ í”„ë¡œì„¸ìŠ¤ê°€ í•´ë‹¹ í´ë”ì— ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë„ë¡ ì†Œìœ ì ë³€ê²½\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jjugup6tEF6",
        "outputId": "eb1c542a-c70f-4d1d-b9c2-beee051d3757"
      },
      "source": [
        "### íŒŒì´ì¬ í™˜ê²½ì—ì„œ êµ¬ë™ì„ ìœ„í•œ elasticsearch íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install elasticsearch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/68/76c5d46cc6a48fddb759f585bc8728caa11bfc9b812ce6705fc5f99beab2/elasticsearch-7.11.0-py2.py3-none-any.whl (325kB)\n",
            "\r\u001b[K     |â–ˆ                               | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆ                              | 20kB 21.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆ                             | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆ                            | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 102kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 112kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 122kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 133kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 143kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 153kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 163kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 174kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 184kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 194kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 204kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 215kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 225kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 235kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 245kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 256kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 266kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 276kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 286kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 296kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 317kB 4.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2020.12.5)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDzbifJStJmw"
      },
      "source": [
        "# ë°ëª¬ í”„ë¡œì„¸ìŠ¤ë¡œ ì—˜ë¼ìŠ¤í‹± ì„œë²„ ê°œì‹œí•˜ê¸°\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es = Popen(['elasticsearch-7.0.0/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zg-u5fntcFT"
      },
      "source": [
        "### **3ë¶„ ì •ë„ ê¸°ë‹¤ë¦¬ì‹œë©´ ë©ë‹ˆë‹¤!!!** ğŸ˜„\n",
        "\n",
        "ì„œë²„ê°€ ì•ˆì •ì ìœ¼ë¡œ ì—´ë¦´ ë•Œê¹Œì§€ ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”..\n",
        "\n",
        "`es.info()`ë¥¼ ì‹¤í–‰ì‹œì¼°ì„ ë•Œ\n",
        "\n",
        "ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ì„±ê³µ!\n",
        "\n",
        "```\n",
        "{'cluster_name': 'elasticsearch',\n",
        " 'cluster_uuid': 'EOh60gEwRri9A_UruHyQLA',\n",
        " 'name': '657b2f379e3e',\n",
        " 'tagline': 'You Know, for Search',\n",
        " 'version': {'build_date': '2019-04-05T22:55:32.697037Z',\n",
        "  'build_flavor': 'default',\n",
        "  'build_hash': 'b7e28a7',\n",
        "  'build_snapshot': False,\n",
        "  'build_type': 'tar',\n",
        "  'lucene_version': '8.0.0',\n",
        "  'minimum_index_compatibility_version': '6.0.0-beta1',\n",
        "  'minimum_wire_compatibility_version': '6.7.0',\n",
        "  'number': '7.0.0'}}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZD2PfkXt9Xy",
        "outputId": "8a74066d-b8b3-471b-f2fd-d763db884d29"
      },
      "source": [
        "# ë¡œì»¬ ì„œë²„ì— ì—˜ë¼ìŠ¤í‹± ì„œë²„ì™€ pythonì„ ì—°ê²°\n",
        "from elasticsearch import Elasticsearch\n",
        "es = Elasticsearch(\"localhost:9200/\")\n",
        "es.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cluster_name': 'elasticsearch',\n",
              " 'cluster_uuid': 'GgL6IF_NRlOFMC9cIxTEUw',\n",
              " 'name': 'ef098ec87dba',\n",
              " 'tagline': 'You Know, for Search',\n",
              " 'version': {'build_date': '2019-04-05T22:55:32.697037Z',\n",
              "  'build_flavor': 'default',\n",
              "  'build_hash': 'b7e28a7',\n",
              "  'build_snapshot': False,\n",
              "  'build_type': 'tar',\n",
              "  'lucene_version': '8.0.0',\n",
              "  'minimum_index_compatibility_version': '6.0.0-beta1',\n",
              "  'minimum_wire_compatibility_version': '6.7.0',\n",
              "  'number': '7.0.0'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqTiE1R6vjh9"
      },
      "source": [
        "### **Elasticsearch Server ê°œì‹œ ë° Pythonê³¼ ì—°ë™**\n",
        "\n",
        "> /content/gdrive ì´í›„ì˜ ê²½ë¡œì—ëŠ” êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë³´ì•ˆìœ¼ë¡œ ì¸í•´ì„œ\n",
        "daemon processê°€ ì •ìƒì ì¸ ë°©ì‹ìœ¼ë¡œëŠ” ì„œë²„ êµ¬ë™ì´ ì–´ë µê¸° ë•Œë¬¸ì— ë³¸ì¸ ë“œë¼ì´ë¸Œê°€ ì•„ë‹Œ\n",
        "êµ¬ê¸€ í´ë¼ìš°ë“œ ì»´í“¨í„°ì— ì—˜ë¼ìŠ¤í‹± ì„œë²„ë¥¼ ì„¤ì¹˜í•˜ê³  ê°œì‹œí–ˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsuXyBkVZPxA"
      },
      "source": [
        "### í˜„ì¬ ì‘ì—… í™˜ê²½ ì¬ì„¤ì •í•˜ê¸° (ìì‹ ë§Œì˜ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ë³¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ëŠ” ê²ƒ)\n",
        "\n",
        "#.ipynb ë…¸íŠ¸ë¶ íŒŒì¼ê³¼ ë¶ˆëŸ¬ì˜¤ë ¤ëŠ” ì´ë¯¸ì§€ ë° ëª¨ë“ˆì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ë“¤ì€ ê°™ì€ í˜„ì¬ ì‘ì—… í™˜ê²½ \n",
        "#ë˜ëŠ” ê·¸ í•˜ìœ„ í´ë”ì— ì¡´ì¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \n",
        "\n",
        "# ìì‹ ë§Œì˜ ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •í•˜ëŠ”ë°, ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ëŠ” Colab Notebooksê¹Œì§€ ë™ì¼í•˜ê²Œ ê³µìœ í•˜ë©°,\n",
        "# ì´í›„ ê²½ë¡œëŠ” ìì‹ ì´ ìƒì„±í•œ í´ë”ì— ë§ì¶”ì–´ì„œ ê²½ë¡œë¥¼ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.\n",
        "# ìˆ˜ì—…ì—ì„œëŠ” ìì‹ ì˜ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— information_retrieval í´ë”ë¥¼ ìƒì„±í•˜ì‹œë©´ ë©ë‹ˆë‹¤. \n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TOmX0hPZdS0"
      },
      "source": [
        "### ğŸ¤¥ğŸ’¦ ëª¨ë¥´ëŠ” ê²ƒì´ ìˆì„ ê²½ìš°ì—ëŠ”?\n",
        "\n",
        "í”„ë¡œê·¸ë˜ë°ì˜ ì²« ê±¸ìŒì€ **`ê²€ìƒ‰`** ì…ë‹ˆë‹¤.\n",
        "\n",
        "íŠ¹íˆ **`ë¹…ë°ì´í„°ì™€ ì •ë³´ ê²€ìƒ‰`** ìˆ˜ì—…ì„ ë“£ê³  ìˆëŠ” ì—¬ëŸ¬ë¶„ë“¤ì€ **ë°˜.ë“œ.ì‹œ** ê¶ê¸ˆí•˜ê±°ë‚˜ ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œ **`ê²€ìƒ‰`**í•˜ëŠ” ìŠµê´€ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "\n",
        "> Google ê²€ìƒ‰ ì—”ì§„ \n",
        "                \n",
        "    -> \"Python ë£¨í”„ë¬¸\"\n",
        "                 \n",
        "    -> \"Python forë¬¸ ì—ëŸ¬\"\n",
        "\n",
        "    -> \"What is SyntaxError: invalid syntax in python?\"\n",
        "\n",
        "ë“±ì˜ `ì—ëŸ¬ë¬¸ì¥`ê³¼ `í‚¤ì›Œë“œ`ë¥¼ ì ê·¹ í™œìš©í•´ì£¼ì„¸ìš”!ğŸ˜„\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDsLe1QI1QDo"
      },
      "source": [
        "### **ë°ì´í„°ì— ëŒ€í•´ì„œ ìƒˆë¡­ê²Œ ì¸ë±ì‹±í•˜ê¸°** ğŸ˜„\n",
        "\n",
        "> ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì˜ ë°ì´í„°ë¡œ ê°„ë‹¨í•˜ê²Œ ì—°ìŠµí•´ë³´ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlUp5Tsw1NhC",
        "outputId": "c5a9a297-3235-4081-e809-ecb0d34a95da"
      },
      "source": [
        "# ë°ì´í„°ë¥¼ ìƒ‰ì¸í™”í•˜ê¸° ìœ„í•œ í•¨ìˆ˜\n",
        "def indexing(es, index_name):\n",
        "    # ì´ë¯¸ ì¡´ì¬í•  ê²½ìš° ì‚­ì œí•˜ê³  ë‹¤ì‹œ ë§Œë“¤ê¸°\n",
        "    if es.indices.exists(index=index_name):\n",
        "        es.indices.delete(index=index_name)\n",
        "\n",
        "    # ì¸ë±ìŠ¤ ìƒì„±\n",
        "    print(es.indices.create(index=index_name))\n",
        "\n",
        "# ì¸ë±ìŠ¤ëª…ì„ ì •í•˜ê¸° (ììœ ë¡­ê²Œ)\n",
        "index_name = 'sample_index'\n",
        "indexing(es, index_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'sample_index'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68zDcAB6-rkF"
      },
      "source": [
        "### **ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ì—ì„œ ìƒ‰ì¸í™”ëŠ” ì–´ë–»ê²Œ í• ê¹Œìš”?** ğŸ˜Š\n",
        "\n",
        "> ë³€ìˆ˜ëª…, key, valueì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì€ ììœ ë¡­ê²Œ ì‘ì„±í•˜ë©´ ë©ë‹ˆë‹¤. ë”•ì…”ë„ˆë¦¬ ìë£Œí˜•ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì—, í•´ë‹¹ í˜•ì‹ì— ë§ì¶”ì–´ì„œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f96zbclC_kG7",
        "outputId": "f949dbae-25de-40f9-cb18-b631c764784f"
      },
      "source": [
        "### ìƒˆë¡œìš´ ë°ì´í„° ì¸ë±ì‹± \n",
        "## ì—¬ê¸°ì„œëŠ” 'ë…¸ë˜ ì œëª©'ê³¼ 'ê°€ì‚¬ ì¼ë¶€'ë¥¼ mappingí–ˆìŠµë‹ˆë‹¤.\n",
        "sample1= {'songs': 'ë¯¼ì¡±ì˜ ì•„ë¦¬ì•„', 'lyrics': 'ì§€ì¶•ì„ ë°•ì°¨ê³  í¬íš¨í•˜ë¼ ê·¸ëŒ€'}\n",
        "sample2= {'songs': 'ë±ƒë…¸ë˜', 'lyrics': 'ì¦ê±°ìš´ ê³ ì—°ì „ ë‚ ì— ì§€ê³  ê°€ëŠ” ì—°ëŒ€ìƒì´ ì²˜ëŸ‰ë„ í•˜êµ¬ë‚˜'}\n",
        "sample3= {'songs': 'ì„íƒ‘', 'lyrics': 'ì´ë¦„ëª¨ë¥¼ ì„ê³µì˜ ë•€ê³¼ ëˆˆë¬¼ì´ í˜ëŸ¬ë‚´ë¦¬ë„¤'}\n",
        "sample4= {'songs': 'Forever', 'lyrics': 'ìš°ë¦¬ì˜ í•¨ì„±ì€ ì‹ í™”ê°€ ë˜ë¦¬ë¼ ìš¸ë ¤ë¼ ì´ê³³ì— Forever'}\n",
        "sample5= {'songs': 'ì Šì€ê·¸ëŒ€', 'lyrics': 'ì‚¬ë‘ìŠ¤ëŸ° ì Šì€ ê·¸ëŒ€ íƒœì–‘ê°™ì€ ì Šì€ ê·¸ëŒ€'}\n",
        "\n",
        "# es ë³€ìˆ˜ì— ê°ì²´í™”í•œ ì„œë²„ëŠ” ìœ„ì—ì„œ ì •ì˜í•œ ë°ì•„í„°ì— ëŒ€í•´ì„œ\n",
        "# ì´ì „ ì…€ì—ì„œ ì„ ì–¸í•œ 'sample_index'ë¡œ ì¸ë±ìŠ¤ëª…ì„ ì •í•˜ê³ \n",
        "# ë°ì´í„°ì˜ íƒ€ì…ì€ ë¬¸ìì—´ì¸ ìƒíƒœë¡œ ìƒ‰ì¸í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "es.index(index=index_name, doc_type='string', body = sample1)\n",
        "es.index(index=index_name, doc_type='string', body = sample2)\n",
        "es.index(index=index_name, doc_type='string', body = sample3)\n",
        "es.index(index=index_name, doc_type='string', body = sample4)\n",
        "es.index(index=index_name, doc_type='string', body = sample5)\n",
        "\n",
        "es.indices.refresh(index=index_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
            "  warnings.warn(message, category=ElasticsearchWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_shards': {'failed': 0, 'successful': 1, 'total': 2}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17hSJcKEAJY7"
      },
      "source": [
        "### **ìƒ‰ì¸í™”í•œ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì •ë³´ ê²€ìƒ‰í•´ë³´ê¸°** ğŸ¤©\n",
        "\n",
        "> es.searchë¥¼ í™œìš©í•´ì„œ ê²€ìƒ‰í•´ë³´ê¸°\n",
        "\n",
        "```\n",
        "es.search(index = 'ì„ ì–¸í•œ ì¸ë±ìŠ¤ëª…', body = {'from': 'ëª‡ ìœ„ë¶€í„° ë°˜í™˜í•  ê²ƒì¸ì§€' , \n",
        "'size': 'ìµœëŒ€ ë°˜í™˜í•  ê°¯ìˆ˜', 'query' : {'match':{'ê¸°ì¤€ key' : 'ê²€ìƒ‰í•  ë‚´ìš©'}}})\n",
        "```\n",
        "\n",
        "### ê²°ê³¼ë¡œëŠ” ë¬´ì—‡ì´ ë°˜í™˜ë˜ì—ˆì„ê¹Œ...? ğŸ§\n",
        "\n",
        "> ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ, í•„ìš”í•œ ì •ë³´ëŠ” ë”•ì…”ë„ˆë¦¬ valueì— ëŒ€í•œ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "```\n",
        "{'took': 3, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, \n",
        "'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 2, 'relation': 'eq'},\n",
        "'max_score': 1.2037696, 'hits': [{'_index': 'sample_index', '_type': 'string',\n",
        "'_id': 'zYMJ9HcB0qdz_4P6yRNV', '_score': 1.2037696, '_source': \n",
        "{'songs': 'ì Šì€ê·¸ëŒ€', 'lyrics': 'ì‚¬ë‘ìŠ¤ëŸ° ì Šì€ ê·¸ëŒ€ íƒœì–‘ê°™ì€ ì Šì€ ê·¸ëŒ€'}}, \n",
        "{'_index': 'sample_index', '_type': 'string', '_id': 'yYMJ9HcB0qdz_4P6yBPd',\n",
        "'_score': 1.0137007, '_source': {'songs': 'ë¯¼ì¡±ì˜ ì•„ë¦¬ì•„', 'lyrics': 'ì§€ì¶•ì„ ë°•ì°¨ê³  \n",
        "í¬íš¨í•˜ë¼ ê·¸ëŒ€'}}]}}\n",
        "```\n",
        "\n",
        "> ë”•ì…”ë„ˆë¦¬ì˜ ì ‘ê·¼ì€..?\n",
        "\n",
        "```\n",
        "ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ëª…['key'] # valueë¥¼ ë°˜í™˜! \n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3r0EBhuEFgy",
        "outputId": "16a213a2-3c99-4869-d5ea-8d58891cb0ae"
      },
      "source": [
        "# ê²€ìƒ‰ í•´ë³´ê¸°\n",
        "results = es.search(index=index_name, body={'from':0, 'size':10, 'query':{'match':{'lyrics':'ê·¸ëŒ€'}}})\n",
        "\n",
        "# ë”•ì…”ë„ˆë¦¬ ì•ˆì— ë”•ì…”ë„ˆë¦¬: 'hits'ë¼ëŠ” key ì•ˆì— valueëŠ” ë˜ ë‹¤ë¥¸ ì‚¬ì „í˜•ìœ¼ë¡œ ë˜ì–´ìˆìœ¼ë©°, í•´ë‹¹ ì‚¬ì „ì˜ key ì´ë¦„ì€ 'hits'   \n",
        "for result in results['hits']['hits']:\n",
        "    print('score:', result['_score'], 'source::', result['_source'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score: 1.2037696 source:: {'songs': 'ì Šì€ê·¸ëŒ€', 'lyrics': 'ì‚¬ë‘ìŠ¤ëŸ° ì Šì€ ê·¸ëŒ€ íƒœì–‘ê°™ì€ ì Šì€ ê·¸ëŒ€'}\n",
            "score: 1.0137007 source:: {'songs': 'ë¯¼ì¡±ì˜ ì•„ë¦¬ì•„', 'lyrics': 'ì§€ì¶•ì„ ë°•ì°¨ê³  í¬íš¨í•˜ë¼ ê·¸ëŒ€'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVT31VFMFxFF"
      },
      "source": [
        "### **ìœ„í‚¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì¸ë±ì‹±í•˜ê¸°** â˜ºï¸\n",
        "\n",
        "> ì˜ì–´ ë‚˜ë¬´ìœ„í‚¤ ë¤í”„ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì€ ë‹¤ìŒì— ì „ì²˜ë¦¬í•˜ì—¬ ì‚¬ìš©í•´ë³´ê¸°\n",
        "\n",
        "https://dumps.wikimedia.org/enwiki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aDqvMLNg7jC",
        "outputId": "286989f5-3e2a-4c46-e163-9fd9830312c0"
      },
      "source": [
        "# ìƒ˜í”Œ ì˜ì–´ ìœ„í‚¤ ë¤í”„ë¥¼ ë‹¤ìš´ë¡œë“œ\n",
        "!wget https://github.com/AlonEirew/wikipedia-to-elastic/raw/master/dumps/tinywiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-03 05:16:07--  https://github.com/AlonEirew/wikipedia-to-elastic/raw/master/dumps/tinywiki-latest-pages-articles.xml.bz2\n",
            "Resolving github.com (github.com)... 13.114.40.48\n",
            "Connecting to github.com (github.com)|13.114.40.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AlonEirew/wikipedia-to-elastic/master/dumps/tinywiki-latest-pages-articles.xml.bz2 [following]\n",
            "--2021-03-03 05:16:07--  https://raw.githubusercontent.com/AlonEirew/wikipedia-to-elastic/master/dumps/tinywiki-latest-pages-articles.xml.bz2\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9731 (9.5K) [application/octet-stream]\n",
            "Saving to: â€˜tinywiki-latest-pages-articles.xml.bz2â€™\n",
            "\n",
            "tinywiki-latest-pag 100%[===================>]   9.50K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-03-03 05:16:07 (7.91 MB/s) - â€˜tinywiki-latest-pages-articles.xml.bz2â€™ saved [9731/9731]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWTWx1B18DU0"
      },
      "source": [
        "### **WikiExtractorë¥¼ ì‚¬ìš©í•´ì„œ ì˜ì–´ ìœ„í‚¤ë°ì´í„° ì „ì²˜ë¦¬** ğŸ‘\n",
        "\n",
        "<img src = \"https://avatars.githubusercontent.com/u/356002?s=460&v=4\" width = \"300\" >\n",
        "\n",
        "> WikiExtractor https://github.com/attardi/wikiextractor\n",
        "\n",
        "```\n",
        "@misc{Wikiextractor2015,\n",
        "  author = {Giusepppe Attardi},\n",
        "  title = {WikiExtractor},\n",
        "  year = {2015},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{https://github.com/attardi/wikiextractor}}\n",
        "}\n",
        "```\n",
        "\n",
        "ìœ„í‚¤ ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆëŠ” ë¶ˆí•„ìš”í•œ íƒœê·¸, íŠ¹ìˆ˜ ë¬¸ì, ê¸°í˜¸, ìˆ«ì ë“±ì„ ì œê±°í•˜ê³ \n",
        "\n",
        "ì›í•˜ëŠ” ë¶€ë¶„ì„ ë³„ë„ë¡œ ì¶”ì¶œì´ ê°€ëŠ¥ ex) ë¬¸ì„œ ë³¸ë¬¸, ì œëª©, ì €ì ë“±\n",
        "\n",
        "ì‹¤ìŠµì—ì„œëŠ” ë¬¸ì„œì˜ ì œëª©ê³¼ ë³¸ë¬¸ ìœ„ì£¼ë¡œ íŒŒì‹±ì„ ì§„í–‰.\n",
        "\n",
        "*ë” ìì„¸í•œ ë‚´ìš©ì€ ìœ„ì˜ ì €ì¥ì†Œ ì£¼ì†Œë¥¼ í†µí•´ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jijQiwAv2oaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1018e635-1c51-48d0-c089-196da8a8ac35"
      },
      "source": [
        "# ìœ„í‚¤ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  json í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” ì½”ë“œë¥¼ ì°¸ì¡°\n",
        "!git clone https://github.com/attardi/wikiextractor.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wikiextractor'...\n",
            "remote: Enumerating objects: 733, done.\u001b[K\n",
            "remote: Total 733 (delta 0), reused 0 (delta 0), pack-reused 733\u001b[K\n",
            "Receiving objects: 100% (733/733), 1.28 MiB | 4.75 MiB/s, done.\n",
            "Resolving deltas: 100% (427/427), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfm7Hrs6xOaT",
        "outputId": "15fce260-5471-4090-9fc8-634479758675"
      },
      "source": [
        "# ë‹¤ìš´ë¡œë“œ ë°›ì€ ìƒ˜í”Œ ìœ„í‚¤ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ê²€ìƒ‰ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
        "# ê²°ê³¼ëŠ” elastic í´ë”ì— 'extract_result/AA'ë¼ëŠ” ìƒˆë¡œìš´ í´ë”ì— ì €ì¥ëœë‹¤.\n",
        "!python -m wikiextractor.wikiextractor.WikiExtractor tinywiki-latest-pages-articles.xml.bz2 --json -o extract_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Preprocessing 'tinywiki-latest-pages-articles.xml.bz2' to collect template definitions: this may take some time.\n",
            "INFO: Loaded 0 templates in 0.0s\n",
            "INFO: Starting page extraction from tinywiki-latest-pages-articles.xml.bz2.\n",
            "INFO: Using 1 extract processes.\n",
            "INFO: Finished 1-process extraction of 7 articles in 0.0s (159.5 art/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WInyOqC7-WI"
      },
      "source": [
        "### **ì „ì²˜ë¦¬í•œ ìœ„í‚¤ë°ì´í„°ë¡œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ë§Œë“¤ê¸°** ğŸ§\n",
        "\n",
        "<img src = \"https://github.com/facebookresearch/DrQA/raw/master/img/drqa.png\" width = \"600\" >\n",
        "\n",
        "> This is a PyTorch implementation of the DrQA system described in the ACL 2017 paper Reading Wikipedia to Answer Open-Domain Questions.\n",
        "\n",
        "> ACL 2017. í˜ì´ìŠ¤ë¶ì—ì„œ ê³µê°œí•œ ì •ë³´ ê²€ìƒ‰ ê¸°ë°˜ ì§ˆì˜ ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. \n",
        "\n",
        "> ë…¼ë¬¸ ë§í¬: https://arxiv.org/abs/1704.00051\n",
        "\n",
        "ë³¸ ì‹¤ìŠµì€ ìœ„ì˜ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ë°©ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@inproceedings{chen2017reading,\n",
        "  title={Reading {Wikipedia} to Answer Open-Domain Questions},\n",
        "  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},\n",
        "  booktitle={Association for Computational Linguistics (ACL)},\n",
        "  year={2017}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOnyt1naEoug",
        "outputId": "31ba4c5e-d648-442e-e5a5-ea6905a3632f"
      },
      "source": [
        "# ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ ì„œë²„ì™€ ë§ˆì°¬ê°€ì§€ë¡œ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë‚´ë¶€ ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ ì¸í•´,\n",
        "# DRQAì˜ í•„ìˆ˜ ì„¤ì¹˜ íŒ¨í‚¤ì§€ì¸ CoreNLP ì„œë²„ ì—°ë™ì„ ìœ„í•´ì„œ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë°”ê¹¥ì˜ êµ¬ê¸€ í´ë¼ìš°ë“œ ì»´í“¨í„°ì— ì €ì¥ì†Œ ì½”ë“œë¥¼ ë‹¤ìš´ ë°›ìŠµë‹ˆë‹¤.\n",
        "os.chdir('/content/elasticsearch/')\n",
        "!git clone https://github.com/facebookresearch/DrQA.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DrQA'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Total 265 (delta 0), reused 0 (delta 0), pack-reused 265\u001b[K\n",
            "Receiving objects: 100% (265/265), 562.37 KiB | 1.45 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3se7ZZC2E2A0",
        "outputId": "1b0dd307-95c0-47da-8ac1-5a201522a99d"
      },
      "source": [
        "# ë‚´ë ¤ë°›ì€ ì €ì¥ì†Œ íŒ¨í‚¤ì§€ ì•ˆì— í•„ìˆ˜ í•­ëª© ì„¤ì¹˜ \n",
        "os.chdir('/content/elasticsearch/DrQA')\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (7.11.0)\n",
            "Requirement already satisfied: pexpect==4.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->-r requirements.txt (line 6)) (0.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from prettytable->-r requirements.txt (line 6)) (54.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r requirements.txt (line 9)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect==4.2.1->-r requirements.txt (line 10)) (0.7.0)\n",
            "running develop\n",
            "running egg_info\n",
            "writing drqa.egg-info/PKG-INFO\n",
            "writing dependency_links to drqa.egg-info/dependency_links.txt\n",
            "writing requirements to drqa.egg-info/requires.txt\n",
            "writing top-level names to drqa.egg-info/top_level.txt\n",
            "writing manifest file 'drqa.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.7/dist-packages/drqa.egg-link (link to .)\n",
            "drqa 0.1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /content/elasticsearch/DrQA\n",
            "Processing dependencies for drqa==0.1.0\n",
            "Searching for pexpect==4.2.1\n",
            "Best match: pexpect 4.2.1\n",
            "Adding pexpect 4.2.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for elasticsearch==7.11.0\n",
            "Best match: elasticsearch 7.11.0\n",
            "Adding elasticsearch 7.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for nltk==3.2.5\n",
            "Best match: nltk 3.2.5\n",
            "Adding nltk 3.2.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for prettytable==2.0.0\n",
            "Best match: prettytable 2.0.0\n",
            "Adding prettytable 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tqdm==4.41.1\n",
            "Best match: tqdm 4.41.1\n",
            "Adding tqdm 4.41.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for regex==2019.12.20\n",
            "Best match: regex 2019.12.20\n",
            "Adding regex 2019.12.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for ptyprocess==0.7.0\n",
            "Best match: ptyprocess 0.7.0\n",
            "Adding ptyprocess 0.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for certifi==2020.12.5\n",
            "Best match: certifi 2020.12.5\n",
            "Adding certifi 2020.12.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for wcwidth==0.2.5\n",
            "Best match: wcwidth 0.2.5\n",
            "Adding wcwidth 0.2.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==54.0.0\n",
            "Best match: setuptools 54.0.0\n",
            "Adding setuptools 54.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for joblib==1.0.1\n",
            "Best match: joblib 1.0.1\n",
            "Adding joblib 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for drqa==0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhQQezZpH92F"
      },
      "source": [
        "### Standford CoreNLP ì„¤ì¹˜\n",
        "\n",
        "> ì˜ì–´ ìì—°ì–´ì²˜ë¦¬ì— ìœ ìš©í•œ íˆ´ì„ ì œê³µí•˜ëŠ” íŒ¨í‚¤ì§€\n",
        "\n",
        "> ì°¸ì¡°: https://stanfordnlp.github.io/CoreNLP/\n",
        "\n",
        "<img src = \"https://stanfordnlp.github.io/CoreNLP/assets/images/pipeline.png\" width = \"600\" >\n",
        "\n",
        "#### **ì¤‘ìš”!!** ğŸ˜\n",
        "\n",
        "(1) ì„¤ì¹˜ ì‹œì˜ ì•„ë˜ì™€ ê°™ì€ í•­ëª©ì´ ë‚˜ì˜¬ ê²½ìš° `enter` ë˜ëŠ” `data/corenlp`ë¥¼ ì…ë ¥\n",
        "\n",
        "```\n",
        "Specify download path or enter to use default (data/corenlp): 'ì´ ë¶€ë¶„!'\n",
        "```\n",
        "\n",
        "(2) ì„¤ì¹˜ ì‹œì˜ ì•„ë˜ì™€ ê°™ì€ í•­ëª©ì´ ë‚˜ì˜¬ ê²½ìš° `yes`ë¥¼ ì…ë ¥\n",
        "```\n",
        "/content/elasticsearch/DrQA Add to ~/.bashrc CLASSPATH (recommended)? [yes/no]: yes\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4Ab2-pdF5ZF",
        "outputId": "6cd764c9-7494-49a2-a32b-aaa111b35112"
      },
      "source": [
        "# CoreNLP ì„¤ì¹˜\n",
        "!./install_corenlp.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specify download path or enter to use default (data/corenlp): data/corenlp\n",
            "Will download to: data/corenlp\n",
            "/tmp /content/elasticsearch/DrQA\n",
            "--2021-03-03 05:21:52--  http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip [following]\n",
            "--2021-03-03 05:21:53--  https://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 390211140 (372M) [application/zip]\n",
            "Saving to: â€˜stanford-corenlp-full-2017-06-09.zipâ€™\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 372.13M  3.05MB/s    in 1m 41s  \n",
            "\n",
            "2021-03-03 05:23:35 (3.67 MB/s) - â€˜stanford-corenlp-full-2017-06-09.zipâ€™ saved [390211140/390211140]\n",
            "\n",
            "Archive:  stanford-corenlp-full-2017-06-09.zip\n",
            "   creating: stanford-corenlp-full-2017-06-09/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/README.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2017-06-09/sutime/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/sutime/english.holidays.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-javadoc.jar  \n",
            " extracting: stanford-corenlp-full-2017-06-09/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/build.xml  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/pom.xml  \n",
            "   creating: stanford-corenlp-full-2017-06-09/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2017-06-09/patterns/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2017-06-09/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2017-06-09/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2017-06-09/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/input.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/Makefile  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/LICENSE.txt  \n",
            "/content/elasticsearch/DrQA\n",
            "Add to ~/.bashrc CLASSPATH (recommended)? [yes/no]: yes\n",
            "\n",
            "*** NOW RUN: ***\n",
            "\n",
            "export CLASSPATH=$CLASSPATH:data/corenlp/*\n",
            "\n",
            "****************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_8p2t8uKkev",
        "outputId": "ab2047eb-d62f-49c4-f349-36f821ec39d0"
      },
      "source": [
        "# python 3.6ê³¼ 3.7ì—ì„œì˜ ì¶©ëŒì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì½”ë“œ\n",
        "# ê°€ë³ê²Œ ì‹¤í–‰í•˜ê³  ë„˜ì–´ê°€ê¸°\n",
        "!sudo apt-get remove python-pexpect python3-pexpect\n",
        "!sudo pip3.7 install --upgrade pexpect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'python3-pexpect' is not installed, so not removed\n",
            "Package 'python-pexpect' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Collecting pexpect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect) (0.7.0)\n",
            "\u001b[31mERROR: drqa 0.1.0 has requirement pexpect==4.2.1, but you'll have pexpect 4.8.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pexpect\n",
            "  Found existing installation: pexpect 4.2.1\n",
            "    Uninstalling pexpect-4.2.1:\n",
            "      Successfully uninstalled pexpect-4.2.1\n",
            "Successfully installed pexpect-4.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX6UtE8dE5Sv",
        "outputId": "e7652e4f-2971-4057-a72b-4ee853b74050"
      },
      "source": [
        "# WikiExtractorì˜ ê²°ê³¼ë¡œ ìƒì„±í•œ .json íŒŒì¼ì„ db íŒŒì¼ë¡œ ë³€ê²½í•˜ì—¬ ìƒ‰ì¸í™” ì¤€ë¹„\n",
        "os.chdir('/content/elasticsearch/DrQA/scripts/retriever/')\n",
        "!python build_db.py '/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic/extract_result/AA/wiki_00' '/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic/sample_wiki.db'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/03/2021 05:30:32 AM: [ Reading into database... ]\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\n",
            "\r0it [00:00, ?it/s]\u001b[A\r1it [00:00, 28.38it/s]\n",
            "\r100% 1/1 [00:00<00:00, 27.69it/s]\n",
            "03/03/2021 05:30:32 AM: [ Read 6 docs. ]\n",
            "03/03/2021 05:30:32 AM: [ Committing... ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNxuJffaMZUn"
      },
      "source": [
        "os.chdir('/content/elasticsearch/DrQA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOh7g5PKtxuv",
        "outputId": "3f0a7e8f-9798-4fc2-d599-1e7309732019"
      },
      "source": [
        "import drqa.tokenizers\n",
        "import drqa.reader\n",
        "\n",
        "from drqa.retriever.doc_db import DocDB\n",
        "from elasticsearch import Elasticsearch\n",
        "from tqdm import tqdm\n",
        "from drqa.tokenizers import CoreNLPTokenizer\n",
        "from drqa.pipeline import DrQA\n",
        "from drqa.retriever import ElasticDocRanker, TfidfDocRanker\n",
        "\n",
        "# ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±\n",
        "def make_index(es, index_name):\n",
        "    if es.indices.exists(index=index_name):\n",
        "        es.indices.delete(index=index_name)\n",
        "    print(es.indices.create(index=index_name))\n",
        "\n",
        "# ì¸ë±ìŠ¤ ëª…ì€ ììœ ë¡­ê²Œ\n",
        "index_name = 'test_index'\n",
        "make_index(es, index_name)\n",
        "\n",
        "# python build_db.pyë¡œ ìƒì„±í•œ dbì˜ ê²½ë¡œ\n",
        "db = DocDB(db_path='/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic/sample_wiki.db')\n",
        "print(len(db.get_doc_ids())) # 6ê°œ\n",
        "\n",
        "# ìƒ‰ì¸í™” ì§„í–‰\n",
        "for idx, doc_id in enumerate(tqdm(db.get_doc_ids())):\n",
        "    doc = {'title': doc_id, 'content': db.get_doc_text(doc_id)}\n",
        "    es.index(index = index_name, doc_type= 'string', body=doc)\n",
        "\n",
        "es.indices.refresh(index=index_name)\n",
        "\n",
        "# ìƒ‰ì¸ëœ ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ 'content'ë¥¼ keyë¡œ í•˜ì—¬, valueì— ì§ˆì˜ë¥¼ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰ (ì—¬ê¸°ì„œëŠ” filmì„ ì§ˆì˜ë¡œ ì‚¬ìš©)\n",
        "results = es.search(index=index_name, body={'from':0, 'size':10, 'query':{'match':{'content':'film'}}})\n",
        "for result in results['hits']['hits']:\n",
        "    print('score:', result['_score'], 'source:', result['_source'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
            "  warnings.warn(message, category=ElasticsearchWarning)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 38.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'test_index'}\n",
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score: 1.8695081 source: {'title': '4', 'content': \"The Royal Cinema is an Art Moderne event venue and cinema in Toronto, Canada. It was built in 1939 and owned by Miss Ray Levinsky.\\nWhen it was built in 1939, it was called The Pylon, with an accompanying large sign at the front of the theatre. It included a roller-skating rink at the rear of the theatre, and a dance hall on the second floor.\\nIn the 1950's the theatre was purchased by Rocco Mastrangelo. In the 1990s, the theatre was renamed 'the Golden Princess'.\\nSince early 2007, Theatre D has owned and operated The Royal.\\nDuring the daytime it operates as a film and television post-production studio. It hosts film festivals including the European Union Film Festival and Japanese Movie Week.\\nThe Royal was featured in the 2013 film The F Word.\\nReferences.\\n \"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho0rQfbcTvYn"
      },
      "source": [
        "## ğŸ‘ ì‹¤ìŠµ ì¢…ë£Œ!! ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤.\r\n",
        "\r\n",
        "## ğŸ¤¥ğŸ’¦ ëª¨ë¥´ëŠ” ê²ƒì´ ìˆì„ ê²½ìš°ì—ëŠ”? (Remind)\r\n",
        "\r\n",
        "í”„ë¡œê·¸ë˜ë°ì˜ ì²« ê±¸ìŒì€ `ê²€ìƒ‰` ì…ë‹ˆë‹¤.\r\n",
        "\r\n",
        "íŠ¹íˆ `ë¹…ë°ì´í„°ì™€ ì •ë³´ ê²€ìƒ‰` ìˆ˜ì—…ì„ ë“£ê³  ìˆëŠ” ì—¬ëŸ¬ë¶„ë“¤ì€ ë°˜.ë“œ.ì‹œ ê¶ê¸ˆí•˜ê±°ë‚˜ ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œ `ê²€ìƒ‰`í•˜ëŠ” ìŠµê´€ì´ í•„ìš”í•©ë‹ˆë‹¤.\r\n",
        "\r\n",
        "> Google ê²€ìƒ‰ ì—”ì§„ \r\n",
        "                \r\n",
        "    -> \"Python ë£¨í”„ë¬¸\"\r\n",
        "                 \r\n",
        "    -> \"Python forë¬¸ ì—ëŸ¬\"\r\n",
        "\r\n",
        "    -> \"What is SyntaxError: invalid syntax in python?\"\r\n",
        "\r\n",
        "ë“±ì˜ `ì—ëŸ¬ë¬¸ì¥`ê³¼ `í‚¤ì›Œë“œ`ë¥¼ ì ê·¹ í™œìš©í•´ì£¼ì„¸ìš”!ğŸ˜„\r\n",
        "\r\n"
      ]
    }
  ]
}