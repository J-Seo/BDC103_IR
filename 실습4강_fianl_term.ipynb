{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "실습4강_fianl_term.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znPoN2nO4kIn"
      },
      "source": [
        "## **실습 4.**\n",
        "\n",
        "본 실습 자료는 **BDC103(00) 빅데이터와정보검색 강의 실습**을 위해 **고려대학교 자연어처리연구실 (NLP & AI Lab)**에서 제작했습니다.\n",
        "\n",
        "☠️ 외부로의 무단 배포를 금지합니다. ☠️\n",
        "\n",
        "```\n",
        "version 1.0 (2021.05.28)\n",
        "created by: 서재형, 임희석 (고려대학교 자연어처리 연구실)\n",
        "email: wolhalang@gmail.com\n",
        "```\n",
        "\n",
        "```\n",
        "HuggingFace의 모델과 학습 방법을 참조해서 제작했습니다.\n",
        "https://huggingface.co/transformers/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XASlXaWq9Bt1"
      },
      "source": [
        "## 수업 시작 전! 😀\n",
        "\n",
        "BDC103_IR 깃헙 저장소에서 **final_test.json**와 **실습 4강.ipynb**를 다운로드 받으시고\n",
        "\n",
        "본인의 구글 드라이브 **Colab Notebooks/information_retrieval** 폴더로 옮겨주세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot7gqgprSGN-"
      },
      "source": [
        "### **구글 드라이브 연동** 😸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUZnBnDPSJBu",
        "outputId": "473ef6b5-ebf1-4f3c-caca-9266d34872bd"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') \n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/information_retrieval') ## 현재 작업 환경으로 설정한 경로를 입력하세요"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enP3kPdn2niG"
      },
      "source": [
        "### **필요한 패키지 불러오기** 😼\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLeCdSkAz-3g",
        "outputId": "d94c318a-bb18-4d20-8482-f8f484e6edcc"
      },
      "source": [
        "# 설치하는 항목들\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "\n",
        "# 로드하는 항목들\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "import collections\n",
        "import json\n",
        "import logging\n",
        "import transformers\n",
        "import easydict\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets, load_metric\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from typing import Optional, Tuple\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Huggingface transformers의 토크나이저, 설정파일, 최적화 함수 등 사용\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_MAPPING,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    SchedulerType,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    set_seed,\n",
        ")\n",
        "from utils_qa import postprocess_qa_predictions\n",
        "from transformers.utils import check_min_version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/f8/ff7cd6e3b400b33dcbbfd31c6c1481678a2b2f669f521ad20053009a9aa3/datasets-1.7.0-py3-none-any.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, xxhash, huggingface-hub, datasets\n",
            "Successfully installed datasets-1.7.0 fsspec-2021.5.0 huggingface-hub-0.0.9 xxhash-2.0.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 6.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "  Found existing installation: huggingface-hub 0.0.9\n",
            "    Uninstalling huggingface-hub-0.0.9:\n",
            "      Successfully uninstalled huggingface-hub-0.0.9\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting accelerate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/fa/d173d923c953d930702066894abf128a7e5258c6f64cf088d2c5a83f46a3/accelerate-0.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hCollecting pyaml>=20.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.8.1+cu101)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Installing collected packages: pyaml, accelerate\n",
            "Successfully installed accelerate-0.3.0 pyaml-20.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t13gpiAf3Gmi"
      },
      "source": [
        "## 📌 Step 1. 캐글 활용하기 ☝️\n",
        "\n",
        "## 1) 캐글 입문 🐣\n",
        "\n",
        "**Kaggle이란?**\n",
        "\n",
        "데이터 분석 및 머신 러닝 등 데이터 사이언스를 주제로 하는 경진 대회 (Competition)을 주관하는 플랫폼.\n",
        "\n",
        "개인 또는 기관에서 데이터 관련 해결 과제를 상금과 함께 의뢰하고, 이에 대해서 전 세계의 데이터 사이언티스트들이 함께 문제를 해결하고 경쟁합니다.\n",
        "\n",
        "Kaggle은 단순 경쟁을 넘어서, 여러 가지 주제에 따른 다양한 데이터에 분석 방법에 있어서 자유로운 공유와 토의가 이루어지면서 입문자 ~ 전문가 모두에게 유익한 정보를 제공하는데 큰 의미를 지닙니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**그렇다면 우리는 무엇을 할 수 있을까?**\n",
        "\n",
        "Kaggle에서 우리는 크게 2가지의 Task를 할 수 있습니다.\n",
        " \n",
        "(1) 지난 경연, Toy 데이터, 또는 파생 데이터 분석\n",
        "--> 추가적인 의뢰에 의해 누군가 상금을 새롭게 제시할 수 있지만, 기본적으로는 **연습 훈련**에 가깝습니다.\n",
        "\n",
        "(2) 경연 참가 \n",
        "--> 어느정도 실력과 기회가 닿았을 때 도전하는 것이 일반적이며, 상금 사냥, 포트 폴리오, 실력 검증, 시험 등을 목표로 하는 **실전**에 가깝습니다\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Kaggle 홈페이지**\n",
        "\n",
        "https://www.kaggle.com/\n",
        "\n",
        "<br>\n",
        "\n",
        "---------\n",
        "\n",
        "**[최초 화면]**\n",
        "\n",
        "- Kaggle 링크를 클릭해서 들어간 이후 자신의 Google 계정으로 로그인한 상태\n",
        "- 좌측의 메뉴바를 클릭하면 아래 사진과 같이 주요 메뉴가 등장합니다.\n",
        "\n",
        "---------\n",
        "\n",
        "**Compete**: 지난, 현재 진행 중인 경연 정보를 확인할 수 있습니다.\n",
        "\n",
        "---------\n",
        "\n",
        "**Datasets**: 분석 가능한 데이터셋과 그에 대한 정보를 확인할 수 있습니다\n",
        "\n",
        "\n",
        "---------\n",
        "\n",
        "**Notebooks**: 특정 데이터셋 및 주제에 대해 작성한 '다른 사람의 코드', '자신이 작성한 코드' 그리고 웹 상의 Kernel을 통해서 자신의 새로운 Notebook 상의 코드를 작성할 수 있습니다.\n",
        "\n",
        "---------\n",
        "\n",
        "**Discuss**: 특정 데이터셋 및 주제에 대한 Q&A와 논의점들을 확인할 수 있습니다.\n",
        "\n",
        "\n",
        "---------\n",
        "\n",
        "**Courses**: Python Tutorial, Pandas, Numpy 등 데이터 사이언스를 위해 필요한 강좌가 개설되어 있습니다. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3eZr1lHJRyB"
      },
      "source": [
        "## 2) 필요한 Dataset을 Kaggle에서 확인해보기 🐥\n",
        "\n",
        "**데이터 또는 주제 검색**\n",
        "\n",
        "상단의 검색 메뉴를 통해서 특정 키워드를 입력하면 관련된 주제 또는 이름을 지닌 데이터셋 정보를 확인합니다.\n",
        "\n",
        "ex) *titanic*\n",
        "\n",
        "https://www.kaggle.com/c/titanic\n",
        "\n",
        "---\n",
        "\n",
        "**필요 데이터셋 또는 주제를 찾았다면?**\n",
        "\n",
        "위의 링크에서 커버 사진 아래의 메뉴 버튼 (ex. Overview)을 클릭한 뒤 스크롤해서 내리다보면 필요한 정보를 얻을 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "**Data**\n",
        "\n",
        "데이터에 대한 설명이 되어 있는 코드북과, 어떠한 목표와 특징을 가지고 제작된 것인지, 그리고 데이터셋에 대한 간단한 미리보기와 함께 다운로드 받을 수 있도록 되어 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "Competition이 아닌 경우, 해당 데이터셋에 대해서 개인 또는 기관 단위에서 특정한 의뢰와 함께 상금을 제시합니다.\n",
        "\n",
        "---\n",
        "\n",
        "**Kernels**\n",
        "\n",
        "해당 데이터셋 또는 주제에 대해 다른 사람들이 작성한 코드와 주석을 볼 수 있습니다. 처음부터 백지 상태에서 모든 것을 작성하는 것보다 선행자들이 분석한 내용을 참고하여 나의 분석 목적과 의도에 맞게 수정해나가는 것도 좋은 방법입니다.\n",
        "\n",
        "(다른 사람의 코드를 해석해서 벤치마크하여 나에게 맞도록 수정하는 것도 절대 쉬운 일이 아닙니다.)\n",
        "\n",
        "---\n",
        "\n",
        "**Discussion**\n",
        "\n",
        "어쩌면 지금 내가하고 있는 고민은 이미 다른 사람들도 똑같이 했었고 이미 해결한 문제일 수 있습니다. 직접 질문을 할 수도 있는 만큼, 자신이 하고 있는 고민이 구글링으로도 해결하기 어려운 일이라면 한번 확인해보는 것도 나쁘지 않습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FClP0s0UN31o"
      },
      "source": [
        "## 3) Competition 참가 🐤\n",
        "\n",
        "**Active / Completed / InClass**\n",
        "\n",
        "현재 진행 중이며, 상금이 걸려 있는 주제와 데이터셋에 무엇이 있는지 확인할 수 있습니다. 또한, 지난 경연과 대학교 강의 등 교육 목적으로 개최한 경연 대회 목록을 확인할 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "**Team**\n",
        "\n",
        "그러나, 전반적으로 사용자가 확인하고 활용할 수 있는 정보에 대해서는 사실상 큰 차이가 없습니다. 다만 여기는 실전!! 팀을 만들어서 경연 대회에 참여할 수 있습니다. (파티 퀘스트?)\n",
        "\n",
        "---\n",
        "\n",
        "**Leaderboard**\n",
        "\n",
        "Competition의 꽃!! 아래에 보이는 리더 보드는 어느 모델, 그리고 어떠한 팀이 가장 잘 했는지 지정한 기준에 따라서 순위를 매깁니다. \n",
        "\n",
        "데이터 사이언스... 머신 러닝.. 대부분의 발전은 이러한 경쟁에서 비롯되었다고 해도 과언이 아닙니다.\n",
        "\n",
        "---\n",
        "\n",
        "**Submit**\n",
        "\n",
        "1번 방법으로 로컬 환경에서 직접 실행한 결과를 담은 정답 csv를 만든 후 업로드할 수 있습니다. \n",
        "\n",
        "2번 방법으로는 앞서 언급했던 Kernels 탭에서, 웹 상에서 작성하고 분석한 결과를 온라인에서 바로 제출할 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sffc6heARZpo"
      },
      "source": [
        "## 4) Kaggle로 Final Term 해결하기 😎\n",
        "\n",
        "**Document Retrieval Question Anwsering**\n",
        "\n",
        "https://www.kaggle.com/c/bdc103-final\n",
        "\n",
        "위의 링크에서 데이터셋을 다운로드 받고,\n",
        "\n",
        "- final_test_sample.json (이번 실습에서는 해당 파일만 사용)\n",
        "- final_term_no_context.json\n",
        "\n",
        "위의 두 개 파일을 자신의 'Google Drive' 상의 'information_retrieval' 폴더에 옮겨보자.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBB818_2evDK"
      },
      "source": [
        "## 명령행 인터페이스, 사전 훈련된 모델을 로드하여 훈련하고 평가하는데까지 필요한 인자를 사용자 정의 인자로 설정하는 내용을 담고 있음.\n",
        "## 사용법: python example.py --dataset_name squad.json .... \n",
        "## 주피터 환경을 위한 인자 설정법 \n",
        "import easydict\n",
        "import string\n",
        "import re\n",
        "def easydict_args():\n",
        "    \n",
        "    args = easydict.EasyDict({\n",
        "        \"dataset_name\": None,\n",
        "        \"dataset_config_name\": 'squad',\n",
        "        \"train_file\": 'final_test_no_context.json',\n",
        "        \"preprocessing_num_workers\": 4,\n",
        "        \"validation_file\": 'final_test_no_context.json',\n",
        "        \"test_file\": 'final_test_no_context.json',\n",
        "        \"test_save_path\" : \"result_final/final_test.csv\",\n",
        "        \"max_seq_length\": 384,\n",
        "        \"pad_to_max_length\": None,\n",
        "        \"model_name_or_path\": 'result/pytorch_model.bin',\n",
        "        \"config_name\": 'bert-base-cased',\n",
        "        \"tokenizer_name\": 'bert-base-cased',\n",
        "        \"use_slow_tokenizer\": None,\n",
        "        \"per_device_train_batch_size\": 4,\n",
        "        \"per_device_eval_batch_size\": 4,\n",
        "        \"learning_rate\": 3e-5,\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"max_train_steps\": None,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"lr_scheduler_type\": 'linear',\n",
        "        \"num_warmup_steps\":0,\n",
        "        \"output_dir\": 'result_final/',\n",
        "        \"seed\": 42,\n",
        "        \"doc_stride\": 128,\n",
        "        \"n_best_size\": 20,\n",
        "        \"null_score_diff_threshold\": 0.0,\n",
        "        \"version_2_with_negative\": False,\n",
        "        \"max_answer_length\": 30,\n",
        "        \"max_train_samples\": None,\n",
        "        \"max_eval_samples\": None,\n",
        "        \"overwrite_cache\": False,\n",
        "        \"max_predict_samples\": None,\n",
        "        \"do_train\": False,\n",
        "        \"do_eval\": True,\n",
        "        \"do_predict\": False,\n",
        "    })\n",
        "\n",
        "    ## 무결성 체크\n",
        "    ## 인자로 전달한 데이터셋 명이 잘못되었거나 확장자 오류가 있을 경우 raise 이하의 메세지를 출력\n",
        "    if (\n",
        "        args.dataset_name is None\n",
        "        and args.train_file is None\n",
        "        and args.validation_file is None\n",
        "        and args.test_file is None\n",
        "    ):\n",
        "        raise ValueError(\"Need either a dataset name or a training/validation/test file.\")\n",
        "    else:\n",
        "        if args.train_file is not None:\n",
        "            extension = args.train_file.split(\".\")[-1]\n",
        "            assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "        if args.validation_file is not None:\n",
        "            extension = args.validation_file.split(\".\")[-1]\n",
        "            assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if args.test_file is not None:\n",
        "            extension = args.test_file.split(\".\")[-1]\n",
        "            assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n",
        "\n",
        "    if args.output_dir is not None:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "    \n",
        "    ## 위에서 인자로 전달받은 값을 args 변수에 담아 반환\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F_VCTNIhbpf"
      },
      "source": [
        "# SQuAD 평가를 위한 전처리 함수\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5QJ5tVbe009"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "def main():\n",
        "    args = easydict_args()\n",
        "\n",
        "    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
        "    ## 가속화 모드를 지원하며, 해당 모드로 병목이 발생하는 메서드 부분을 대체해서 사용가능.\n",
        "    accelerator = Accelerator()\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(accelerator.state)\n",
        "\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    # accelerator.is_local_main_process is only True for one process per machine.\n",
        "    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
        "    if accelerator.is_local_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # If passed along, set the training seed now.\n",
        "    if args.seed is not None:\n",
        "        set_seed(args.seed)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
        "    # 'text' is found. You can easily tweak this behavior (see below).\n",
        "    #\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    \n",
        "    ## Huggingface의 datasets에 존재하는 데이터셋 명을 사용하는 경우에는 저장된 형식에 맞추어서 로드 (위의 링크에서 리스트 참조)\n",
        "    if args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
        "    ## 사용자 정의의 데이터셋을 사용하는 경우에는 아래의 조건문 실행. \n",
        "    else:\n",
        "        data_files = {}\n",
        "        if args.train_file is not None:\n",
        "            data_files[\"train\"] = args.train_file\n",
        "        if args.validation_file is not None:\n",
        "            data_files[\"validation\"] = args.validation_file\n",
        "        if args.test_file is not None:\n",
        "            data_files[\"test\"] = args.test_file\n",
        "        extension = args.train_file.split(\".\")[-1]\n",
        "        raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    ## 지정한 모델의 설정에 맞추어서 환경 구성 요소 로드\n",
        "    ## ex) BERT --> BERTConfig...\n",
        "    if args.config_name:\n",
        "        config = AutoConfig.from_pretrained(args.config_name)\n",
        "    elif args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "    \n",
        "    ## 지정한 모델의 설정에 맞추어서 토크나이저 로드\n",
        "    ## ex) BERT --> BERTTokenizer ... 30,000개의 vocab와 word piece tokenzier를 곁들인...\n",
        "    if args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True)\n",
        "    elif args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    ## 지정한 모델 로드하기.\n",
        "    ## 여기서는 사전 훈련된 모델 상단에 질의응답 TASK를 위한 레이어를 추가하고 미세조정훈련을 하기위해 ForQuestionAnswering을 불러온다.\n",
        "    if args.model_name_or_path:\n",
        "        model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "            config=config,\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Training new model from scratch\")\n",
        "        model = AutoModelForQuestionAnswering.from_config(config)\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # Preprocessing is slighlty different for training and evaluation.\n",
        "    \n",
        "    ## 전처리하기 \n",
        "    column_names = raw_datasets[\"train\"].column_names\n",
        "    print(column_names)\n",
        "    #print(column_names)\n",
        "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
        "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
        "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
        "\n",
        "    # Padding side determines if we do (question|context) or (context|question).\n",
        "    pad_on_right = tokenizer.padding_side == \"right\"\n",
        "\n",
        "    ## 인자로 전달한 최대 입력 시퀀스 길이가 모델의 최대 입력 임베딩 길이보다 길 경우 에러 반환\n",
        "    if args.max_seq_length > tokenizer.model_max_length:\n",
        "        logger.warning(\n",
        "            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
        "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
        "        )\n",
        "\n",
        "    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n",
        "\n",
        "    ##################\n",
        "\n",
        "    # Training preprocessing\n",
        "    ## 훈련 데이터셋에 대한 전처리 작업\n",
        "    ## 패드의 방향을 오른쪽으로 하는 것이 일반적이지만, 최근에는 다른 방향에도 추가할 수 있도록 기능 추가.\n",
        "    \n",
        "    def prepare_train_features(examples):\n",
        "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\n",
        "        # context that overlaps a bit the context of the previous feature.\n",
        "        tokenized_examples = tokenizer(\n",
        "            examples[question_column_name if pad_on_right else context_column_name],\n",
        "            examples[context_column_name if pad_on_right else question_column_name],\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "            max_length=max_seq_length,\n",
        "            stride=args.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\" if args.pad_to_max_length else False,\n",
        "        )\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "        # its corresponding example. This key gives us just that.\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "        # help us compute the start_positions and end_positions.\n",
        "        \n",
        "        ## span의 여부 체크\n",
        "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "        #print(\"offset_mapping\", offset_mapping)\n",
        "\n",
        "        # Let's label those examples!\n",
        "        ## span 가능하도록 본문에서 정답이 존재하는 시작점, 끝점을 매핑하여 저장\n",
        "        tokenized_examples[\"start_positions\"] = []\n",
        "        tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            # We will label impossible answers with the index of the CLS token.\n",
        "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = examples[answer_column_name][sample_index]\n",
        "            # If no answers are given, set the cls_index as answer.\n",
        "            ## answer span이 없을 경우에는 [CLS]로 대체\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Start/end character index of the answer in the text.\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                # Start token index of the current span in the text.\n",
        "                token_start_index = 0\n",
        "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                    token_start_index += 1\n",
        "\n",
        "                # End token index of the current span in the text.\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "                else:\n",
        "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                    while offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "        return tokenized_examples\n",
        "    \n",
        "    ######################\n",
        "    ## 사용자 정의 훈련 데이터를 입력 인자로 사용하는 경우, --do_train을 입력 인자로 반드시 사용해야함.\n",
        "    if \"train\" not in raw_datasets:\n",
        "        raise ValueError(\"--do_train requires a train dataset\")\n",
        "    train_dataset = raw_datasets[\"train\"]\n",
        "    ## 디버깅용\n",
        "    if args.max_train_samples is not None:\n",
        "        # We will select sample from whole data if agument is specified\n",
        "        train_dataset = train_dataset.select(range(args.max_train_samples))\n",
        "    \n",
        "    ## 로드한 사용자 정의 훈련 데이터셋을 입력 데이터로 인코딩\n",
        "    # Create train feature from dataset\n",
        "    train_dataset = train_dataset.map(\n",
        "        prepare_train_features,\n",
        "        batched=True,\n",
        "        num_proc=args.preprocessing_num_workers,\n",
        "        remove_columns=column_names,\n",
        "        load_from_cache_file=not args.overwrite_cache,\n",
        "    )\n",
        "    if args.max_train_samples is not None:\n",
        "        # Number of samples might increase during Feature Creation, We select only specified max samples\n",
        "        train_dataset = train_dataset.select(range(args.max_train_samples))\n",
        "\n",
        "    ######################\n",
        "    ## 위의 훈련 데이터와 동일\n",
        "    # Validation preprocessing\n",
        "    def prepare_validation_features(examples):\n",
        "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\n",
        "        # context that overlaps a bit the context of the previous feature.\n",
        "        tokenized_examples = tokenizer(\n",
        "            examples[question_column_name if pad_on_right else context_column_name],\n",
        "            examples[context_column_name if pad_on_right else question_column_name],\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "            max_length=max_seq_length,\n",
        "            stride=args.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\" if args.pad_to_max_length else False,\n",
        "        )\n",
        "\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "        # its corresponding example. This key gives us just that.\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
        "        # corresponding example_id and we will store the offset mappings.\n",
        "        tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "            context_index = 1 if pad_on_right else 0\n",
        "\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\n",
        "            sample_index = sample_mapping[i]\n",
        "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "            # position is part of the context or not.\n",
        "            tokenized_examples[\"offset_mapping\"][i] = [\n",
        "                (o if sequence_ids[k] == context_index else None)\n",
        "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "            ]\n",
        "\n",
        "        return tokenized_examples\n",
        "\n",
        "    #################\n",
        "    ## 사용자 정의 검증 데이터를 입력 인자로 사용하는 경우, --do_eval을 입력 인자로 반드시 사용해야함.\n",
        "    if \"validation\" not in raw_datasets:\n",
        "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "    eval_examples = raw_datasets[\"validation\"]\n",
        "    if args.max_eval_samples is not None:\n",
        "        # We will select sample from whole data\n",
        "        eval_examples = eval_examples.select(range(args.max_eval_samples))\n",
        "    # Validation Feature Creation\n",
        "    eval_dataset = eval_examples.map(\n",
        "        prepare_validation_features,\n",
        "        batched=True,\n",
        "        num_proc=args.preprocessing_num_workers,\n",
        "        remove_columns=column_names,\n",
        "        load_from_cache_file=not args.overwrite_cache,\n",
        "    )\n",
        "\n",
        "    if args.max_eval_samples is not None:\n",
        "        # During Feature creation dataset samples might increase, we will select required samples again\n",
        "        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
        "\n",
        "    #################\n",
        "    ## 실제 정답을 반환하고 싶을 경우\n",
        "    ## --do_predict을 입력 인자로 전달하는 경우, 사용자 정의 평가 데이터를 입력 인자로 전달해야함. \n",
        "    if args.do_predict:\n",
        "        if \"test\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_examples = raw_datasets[\"test\"]\n",
        "        if args.max_predict_samples is not None:\n",
        "            # We will select sample from whole data\n",
        "            predict_examples = predict_examples.select(range(args.max_predict_samples))\n",
        "        # Predict Feature Creation\n",
        "        predict_dataset = predict_examples.map(\n",
        "            prepare_validation_features,\n",
        "            batched=True,\n",
        "            num_proc=args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not args.overwrite_cache,\n",
        "        )\n",
        "        if args.max_predict_samples is not None:\n",
        "            # During Feature creation dataset samples might increase, we will select required samples again\n",
        "            predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 3):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    #################\n",
        "    ## 입력 인코딩을 완료한 데이터셋에 대해서 batch_size 만큼 로드하기 (pytorch 프레임워크의 장점)\n",
        "    # DataLoaders creation:\n",
        "    if args.pad_to_max_length:\n",
        "        # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
        "        # to tensors.\n",
        "        data_collator = default_data_collator\n",
        "    else:\n",
        "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
        "        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
        "        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
        "        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n",
        "\n",
        "    ## 훈련 데이터셋에 대한 로더 (지정한 배치 사이즈 만큼만 로드해서 훈련하기)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
        "    )\n",
        "\n",
        "    ## 검증 데이터셋에 대한 로더\n",
        "    eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
        "    )\n",
        "\n",
        "    ## 실제 평가를 위해서 정답의 근거를 삭제한 형태로 inference\n",
        "    if args.do_predict:\n",
        "        predict_dataset_for_model = predict_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "        predict_dataloader = DataLoader(\n",
        "            predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
        "        )\n",
        "\n",
        "    ## huggingface에서 제공하는 간편하게 평가 결과를 반환하도록 하는 함수 \n",
        "    ## 깃헙에서 다운로드 받은 utils_qa.py를 참조\n",
        "    # Post-processing:\n",
        "    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
        "        # Post-processing: we match the start logits and end logits to answers in the original context.\n",
        "        predictions = postprocess_qa_predictions(\n",
        "            examples=examples,\n",
        "            features=features,\n",
        "            predictions=predictions,\n",
        "            version_2_with_negative=args.version_2_with_negative,\n",
        "            n_best_size=args.n_best_size,\n",
        "            max_answer_length=args.max_answer_length,\n",
        "            null_score_diff_threshold=args.null_score_diff_threshold,\n",
        "            output_dir=args.output_dir,\n",
        "            prefix=stage,\n",
        "        )\n",
        "        # Format the result to the format the metric expects.\n",
        "        if args.version_2_with_negative:\n",
        "            formatted_predictions = [\n",
        "                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
        "            ]\n",
        "        else:\n",
        "            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
        "            #formatted_predictions = [{\"id\": normalize_answer(k), \"prediction_text\": normalize_answer(v)} for k, v in predictions.items()]\n",
        "            df_pred = pd.DataFrame(formatted_predictions)\n",
        "            df_pred.to_csv(args.test_save_path , index=False, encoding='utf-8')\n",
        "\n",
        "        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
        "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
        "\n",
        "    ## F1 score, Exact match\n",
        "    metric = load_metric(\"squad_v2\" if args.version_2_with_negative else \"squad\")\n",
        "\n",
        "    #################\n",
        "    # Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
        "    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n",
        "        \"\"\"\n",
        "        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
        "        Args:\n",
        "            start_or_end_logits(:obj:`tensor`):\n",
        "                This is the output predictions of the model. We can only enter either start or end logits.\n",
        "            eval_dataset: Evaluation dataset\n",
        "            max_len(:obj:`int`):\n",
        "                The maximum length of the output tensor. ( See the model.eval() part for more details )\n",
        "        \"\"\"\n",
        "\n",
        "        step = 0\n",
        "        # create a numpy array and fill it with -100.\n",
        "        ## pad에 해당하는 logit 값을 -100으로 지정\n",
        "        ## 효율적인 배치 계산을 위한 방법으로 지정한 max sequence length에 메모리 효율을 올리는 방법\n",
        "        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n",
        "        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather\n",
        "        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n",
        "            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n",
        "            # And after every iteration we have to change the step\n",
        "\n",
        "            batch_size = output_logit.shape[0]\n",
        "            cols = output_logit.shape[1]\n",
        "\n",
        "            if step + batch_size < len(dataset):\n",
        "                logits_concat[step : step + batch_size, :cols] = output_logit\n",
        "            else:\n",
        "                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n",
        "\n",
        "            step += batch_size\n",
        "\n",
        "        return logits_concat\n",
        "\n",
        "    # Optimizer\n",
        "    # Split weights in two groups, one with weight decay and the other not.\n",
        "    ## 최적화 과정에서 L2 Loss를 통해서 지나친 과적합을 방지하기 위한 부분. \n",
        "    ## L2 Regularization, 기존 손실 함수 값에 모든 학습 가능한 파라미터를 제곱하여 합하고 \n",
        "    ## 기정한 람다 값만큼 영향도를 설정하는데 이때 람다는 정규화의 세기를 결정하는 하이퍼 파라미터로 \n",
        "    ## 여기서는 weight_decay에 해당\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    ## 아담 최적화 함수 사용\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
        "\n",
        "    # Prepare everything with our `accelerator`.\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader\n",
        "    )\n",
        "\n",
        "    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
        "    # shorter in multiprocess)\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    ## 가상의 미니배치사용. 최종 업데이트는 각 가상 미니 배치의 gradient 값을 모두 고려한 global gradient\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if args.max_train_steps is None:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    else:\n",
        "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    ## scheduler를 통해서 learning rate의 속도 조절.\n",
        "    ## 처음에는 크게, 점점 약해지도록하며 local minimum으로 추정되는 구간에서는 크게 지정한 횟수 만큼 적용 가능.\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=args.lr_scheduler_type,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.num_warmup_steps,\n",
        "        num_training_steps=args.max_train_steps,\n",
        "    )\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    completed_steps = 0\n",
        "\n",
        "    for epoch in range(args.num_train_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "            accelerator.backward(loss)\n",
        "            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                progress_bar.update(1)\n",
        "                completed_steps += 1\n",
        "\n",
        "            if completed_steps >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "    # Validation\n",
        "    all_start_logits = []\n",
        "    all_end_logits = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            start_logits = outputs.start_logits # 모든 토큰 중에서 가장 probability가 높은 시작점\n",
        "            end_logits = outputs.end_logits# 모든 토큰 중에서 가장 probability가 높은 끝점\n",
        "            \n",
        "            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
        "                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
        "                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
        "\n",
        "            all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n",
        "            all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n",
        "\n",
        "    max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
        "\n",
        "    # concatenate the numpy array\n",
        "    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n",
        "    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n",
        "\n",
        "    # delete the list of numpy arrays\n",
        "    del all_start_logits\n",
        "    del all_end_logits\n",
        "\n",
        "    outputs_numpy = (start_logits_concat, end_logits_concat)\n",
        "    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n",
        "    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
        "    logger.info(f\"Evaluation metrics: {eval_metric}\")\n",
        "\n",
        "    # Prediction\n",
        "    '''\n",
        "    if args.do_predict:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        all_start_logits = []\n",
        "        all_end_logits = []\n",
        "        for step, batch in enumerate(predict_dataloader):\n",
        "            batch = batch.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "                start_logits = outputs.start_logits\n",
        "                end_logits = outputs.end_logits\n",
        "\n",
        "                if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
        "                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
        "                    end_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
        "\n",
        "                all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n",
        "                all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n",
        "\n",
        "        max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
        "        # concatenate the numpy array\n",
        "        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n",
        "        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n",
        "\n",
        "        # delete the list of numpy arrays\n",
        "        del all_start_logits\n",
        "        del all_end_logits\n",
        "\n",
        "        outputs_numpy = (start_logits_concat, end_logits_concat)\n",
        "        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n",
        "        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
        "        logger.info(f\"Predict metrics: {predict_metric}\")\n",
        "    '''\n",
        "    #if args.output_dir is not None:\n",
        "        #accelerator.wait_for_everyone()\n",
        "        #unwrapped_model = accelerator.unwrap_model(model)\n",
        "        #unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An3Iy5vMMQ_G"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_gw9huEe6lz"
      },
      "source": [
        "# Tip!\n",
        "'''\n",
        "with open('final_test_no_context.json', 'r', encoding='utf-8') as f:\n",
        "    test_dataset = json.load(f)\n",
        "    for line, retrieved_line in zip(test_dataset['data'], retrieved_data):\n",
        "        line['context'] = retrieved_line # what you retrieved from document-based database\n",
        "\n",
        "with open('final_test_with_retrieved_text.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_dataset, f)\n",
        "    \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrHwIEYcMkL"
      },
      "source": [
        "## 👍 실습 종료!! 정말 고생 많으셨습니다."
      ]
    }
  ]
}