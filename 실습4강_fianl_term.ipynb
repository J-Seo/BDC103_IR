{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ì‹¤ìŠµ4ê°•_fianl_term.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znPoN2nO4kIn"
      },
      "source": [
        "## **ì‹¤ìŠµ 4.**\n",
        "\n",
        "ë³¸ ì‹¤ìŠµ ìë£ŒëŠ” **BDC103(00) ë¹…ë°ì´í„°ì™€ì •ë³´ê²€ìƒ‰ ê°•ì˜ ì‹¤ìŠµ**ì„ ìœ„í•´ **ê³ ë ¤ëŒ€í•™êµ ìì—°ì–´ì²˜ë¦¬ì—°êµ¬ì‹¤ (NLP & AI Lab)**ì—ì„œ ì œì‘í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "â˜ ï¸ ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ë°°í¬ë¥¼ ê¸ˆì§€í•©ë‹ˆë‹¤. â˜ ï¸\n",
        "\n",
        "```\n",
        "version 1.0 (2021.05.28)\n",
        "created by: ì„œì¬í˜•, ì„í¬ì„ (ê³ ë ¤ëŒ€í•™êµ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬ì‹¤)\n",
        "email: wolhalang@gmail.com\n",
        "```\n",
        "\n",
        "```\n",
        "HuggingFaceì˜ ëª¨ë¸ê³¼ í•™ìŠµ ë°©ë²•ì„ ì°¸ì¡°í•´ì„œ ì œì‘í–ˆìŠµë‹ˆë‹¤.\n",
        "https://huggingface.co/transformers/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XASlXaWq9Bt1"
      },
      "source": [
        "## ìˆ˜ì—… ì‹œì‘ ì „! ğŸ˜€\n",
        "\n",
        "BDC103_IR ê¹ƒí—™ ì €ì¥ì†Œì—ì„œ **final_test.json**ì™€ **ì‹¤ìŠµ 4ê°•.ipynb**ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ìœ¼ì‹œê³ \n",
        "\n",
        "ë³¸ì¸ì˜ êµ¬ê¸€ ë“œë¼ì´ë¸Œ **Colab Notebooks/information_retrieval** í´ë”ë¡œ ì˜®ê²¨ì£¼ì„¸ìš”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot7gqgprSGN-"
      },
      "source": [
        "### **êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ë™** ğŸ˜¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUZnBnDPSJBu",
        "outputId": "473ef6b5-ebf1-4f3c-caca-9266d34872bd"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') \n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/information_retrieval') ## í˜„ì¬ ì‘ì—… í™˜ê²½ìœ¼ë¡œ ì„¤ì •í•œ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enP3kPdn2niG"
      },
      "source": [
        "### **í•„ìš”í•œ íŒ¨í‚¤ì§€ ë¶ˆëŸ¬ì˜¤ê¸°** ğŸ˜¼\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLeCdSkAz-3g",
        "outputId": "d94c318a-bb18-4d20-8482-f8f484e6edcc"
      },
      "source": [
        "# ì„¤ì¹˜í•˜ëŠ” í•­ëª©ë“¤\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "\n",
        "# ë¡œë“œí•˜ëŠ” í•­ëª©ë“¤\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "import collections\n",
        "import json\n",
        "import logging\n",
        "import transformers\n",
        "import easydict\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets, load_metric\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from typing import Optional, Tuple\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Huggingface transformersì˜ í† í¬ë‚˜ì´ì €, ì„¤ì •íŒŒì¼, ìµœì í™” í•¨ìˆ˜ ë“± ì‚¬ìš©\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_MAPPING,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    SchedulerType,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    set_seed,\n",
        ")\n",
        "from utils_qa import postprocess_qa_predictions\n",
        "from transformers.utils import check_min_version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/f8/ff7cd6e3b400b33dcbbfd31c6c1481678a2b2f669f521ad20053009a9aa3/datasets-1.7.0-py3-none-any.whl (234kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, xxhash, huggingface-hub, datasets\n",
            "Successfully installed datasets-1.7.0 fsspec-2021.5.0 huggingface-hub-0.0.9 xxhash-2.0.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.3MB 6.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 34.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "  Found existing installation: huggingface-hub 0.0.9\n",
            "    Uninstalling huggingface-hub-0.0.9:\n",
            "      Successfully uninstalled huggingface-hub-0.0.9\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting accelerate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/fa/d173d923c953d930702066894abf128a7e5258c6f64cf088d2c5a83f46a3/accelerate-0.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 2.0MB/s \n",
            "\u001b[?25hCollecting pyaml>=20.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.8.1+cu101)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Installing collected packages: pyaml, accelerate\n",
            "Successfully installed accelerate-0.3.0 pyaml-20.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t13gpiAf3Gmi"
      },
      "source": [
        "## ğŸ“Œ Step 1. ìºê¸€ í™œìš©í•˜ê¸° â˜ï¸\n",
        "\n",
        "## 1) ìºê¸€ ì…ë¬¸ ğŸ£\n",
        "\n",
        "**Kaggleì´ë€?**\n",
        "\n",
        "ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹  ëŸ¬ë‹ ë“± ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ë¥¼ ì£¼ì œë¡œ í•˜ëŠ” ê²½ì§„ ëŒ€íšŒ (Competition)ì„ ì£¼ê´€í•˜ëŠ” í”Œë«í¼.\n",
        "\n",
        "ê°œì¸ ë˜ëŠ” ê¸°ê´€ì—ì„œ ë°ì´í„° ê´€ë ¨ í•´ê²° ê³¼ì œë¥¼ ìƒê¸ˆê³¼ í•¨ê»˜ ì˜ë¢°í•˜ê³ , ì´ì— ëŒ€í•´ì„œ ì „ ì„¸ê³„ì˜ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ë“¤ì´ í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ê²½ìŸí•©ë‹ˆë‹¤.\n",
        "\n",
        "Kaggleì€ ë‹¨ìˆœ ê²½ìŸì„ ë„˜ì–´ì„œ, ì—¬ëŸ¬ ê°€ì§€ ì£¼ì œì— ë”°ë¥¸ ë‹¤ì–‘í•œ ë°ì´í„°ì— ë¶„ì„ ë°©ë²•ì— ìˆì–´ì„œ ììœ ë¡œìš´ ê³µìœ ì™€ í† ì˜ê°€ ì´ë£¨ì–´ì§€ë©´ì„œ ì…ë¬¸ì ~ ì „ë¬¸ê°€ ëª¨ë‘ì—ê²Œ ìœ ìµí•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ë° í° ì˜ë¯¸ë¥¼ ì§€ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "<br>\n",
        "\n",
        "**ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ë¬´ì—‡ì„ í•  ìˆ˜ ìˆì„ê¹Œ?**\n",
        "\n",
        "Kaggleì—ì„œ ìš°ë¦¬ëŠ” í¬ê²Œ 2ê°€ì§€ì˜ Taskë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        " \n",
        "(1) ì§€ë‚œ ê²½ì—°, Toy ë°ì´í„°, ë˜ëŠ” íŒŒìƒ ë°ì´í„° ë¶„ì„\n",
        "--> ì¶”ê°€ì ì¸ ì˜ë¢°ì— ì˜í•´ ëˆ„êµ°ê°€ ìƒê¸ˆì„ ìƒˆë¡­ê²Œ ì œì‹œí•  ìˆ˜ ìˆì§€ë§Œ, ê¸°ë³¸ì ìœ¼ë¡œëŠ” **ì—°ìŠµ í›ˆë ¨**ì— ê°€ê¹ìŠµë‹ˆë‹¤.\n",
        "\n",
        "(2) ê²½ì—° ì°¸ê°€ \n",
        "--> ì–´ëŠì •ë„ ì‹¤ë ¥ê³¼ ê¸°íšŒê°€ ë‹¿ì•˜ì„ ë•Œ ë„ì „í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë©°, ìƒê¸ˆ ì‚¬ëƒ¥, í¬íŠ¸ í´ë¦¬ì˜¤, ì‹¤ë ¥ ê²€ì¦, ì‹œí—˜ ë“±ì„ ëª©í‘œë¡œ í•˜ëŠ” **ì‹¤ì „**ì— ê°€ê¹ìŠµë‹ˆë‹¤\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Kaggle í™ˆí˜ì´ì§€**\n",
        "\n",
        "https://www.kaggle.com/\n",
        "\n",
        "<br>\n",
        "\n",
        "---------\n",
        "\n",
        "**[ìµœì´ˆ í™”ë©´]**\n",
        "\n",
        "- Kaggle ë§í¬ë¥¼ í´ë¦­í•´ì„œ ë“¤ì–´ê°„ ì´í›„ ìì‹ ì˜ Google ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•œ ìƒíƒœ\n",
        "- ì¢Œì¸¡ì˜ ë©”ë‰´ë°”ë¥¼ í´ë¦­í•˜ë©´ ì•„ë˜ ì‚¬ì§„ê³¼ ê°™ì´ ì£¼ìš” ë©”ë‰´ê°€ ë“±ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "---------\n",
        "\n",
        "**Compete**: ì§€ë‚œ, í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ê²½ì—° ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---------\n",
        "\n",
        "**Datasets**: ë¶„ì„ ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ê³¼ ê·¸ì— ëŒ€í•œ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "\n",
        "\n",
        "---------\n",
        "\n",
        "**Notebooks**: íŠ¹ì • ë°ì´í„°ì…‹ ë° ì£¼ì œì— ëŒ€í•´ ì‘ì„±í•œ 'ë‹¤ë¥¸ ì‚¬ëŒì˜ ì½”ë“œ', 'ìì‹ ì´ ì‘ì„±í•œ ì½”ë“œ' ê·¸ë¦¬ê³  ì›¹ ìƒì˜ Kernelì„ í†µí•´ì„œ ìì‹ ì˜ ìƒˆë¡œìš´ Notebook ìƒì˜ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---------\n",
        "\n",
        "**Discuss**: íŠ¹ì • ë°ì´í„°ì…‹ ë° ì£¼ì œì— ëŒ€í•œ Q&Aì™€ ë…¼ì˜ì ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "---------\n",
        "\n",
        "**Courses**: Python Tutorial, Pandas, Numpy ë“± ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ë¥¼ ìœ„í•´ í•„ìš”í•œ ê°•ì¢Œê°€ ê°œì„¤ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3eZr1lHJRyB"
      },
      "source": [
        "## 2) í•„ìš”í•œ Datasetì„ Kaggleì—ì„œ í™•ì¸í•´ë³´ê¸° ğŸ¥\n",
        "\n",
        "**ë°ì´í„° ë˜ëŠ” ì£¼ì œ ê²€ìƒ‰**\n",
        "\n",
        "ìƒë‹¨ì˜ ê²€ìƒ‰ ë©”ë‰´ë¥¼ í†µí•´ì„œ íŠ¹ì • í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ëœ ì£¼ì œ ë˜ëŠ” ì´ë¦„ì„ ì§€ë‹Œ ë°ì´í„°ì…‹ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "ex) *titanic*\n",
        "\n",
        "https://www.kaggle.com/c/titanic\n",
        "\n",
        "---\n",
        "\n",
        "**í•„ìš” ë°ì´í„°ì…‹ ë˜ëŠ” ì£¼ì œë¥¼ ì°¾ì•˜ë‹¤ë©´?**\n",
        "\n",
        "ìœ„ì˜ ë§í¬ì—ì„œ ì»¤ë²„ ì‚¬ì§„ ì•„ë˜ì˜ ë©”ë‰´ ë²„íŠ¼ (ex. Overview)ì„ í´ë¦­í•œ ë’¤ ìŠ¤í¬ë¡¤í•´ì„œ ë‚´ë¦¬ë‹¤ë³´ë©´ í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "**Data**\n",
        "\n",
        "ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ì´ ë˜ì–´ ìˆëŠ” ì½”ë“œë¶ê³¼, ì–´ë– í•œ ëª©í‘œì™€ íŠ¹ì§•ì„ ê°€ì§€ê³  ì œì‘ëœ ê²ƒì¸ì§€, ê·¸ë¦¬ê³  ë°ì´í„°ì…‹ì— ëŒ€í•œ ê°„ë‹¨í•œ ë¯¸ë¦¬ë³´ê¸°ì™€ í•¨ê»˜ ë‹¤ìš´ë¡œë“œ ë°›ì„ ìˆ˜ ìˆë„ë¡ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "Competitionì´ ì•„ë‹Œ ê²½ìš°, í•´ë‹¹ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ê°œì¸ ë˜ëŠ” ê¸°ê´€ ë‹¨ìœ„ì—ì„œ íŠ¹ì •í•œ ì˜ë¢°ì™€ í•¨ê»˜ ìƒê¸ˆì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "**Kernels**\n",
        "\n",
        "í•´ë‹¹ ë°ì´í„°ì…‹ ë˜ëŠ” ì£¼ì œì— ëŒ€í•´ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì‘ì„±í•œ ì½”ë“œì™€ ì£¼ì„ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë°±ì§€ ìƒíƒœì—ì„œ ëª¨ë“  ê²ƒì„ ì‘ì„±í•˜ëŠ” ê²ƒë³´ë‹¤ ì„ í–‰ìë“¤ì´ ë¶„ì„í•œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‚˜ì˜ ë¶„ì„ ëª©ì ê³¼ ì˜ë„ì— ë§ê²Œ ìˆ˜ì •í•´ë‚˜ê°€ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "(ë‹¤ë¥¸ ì‚¬ëŒì˜ ì½”ë“œë¥¼ í•´ì„í•´ì„œ ë²¤ì¹˜ë§ˆí¬í•˜ì—¬ ë‚˜ì—ê²Œ ë§ë„ë¡ ìˆ˜ì •í•˜ëŠ” ê²ƒë„ ì ˆëŒ€ ì‰¬ìš´ ì¼ì´ ì•„ë‹™ë‹ˆë‹¤.)\n",
        "\n",
        "---\n",
        "\n",
        "**Discussion**\n",
        "\n",
        "ì–´ì©Œë©´ ì§€ê¸ˆ ë‚´ê°€í•˜ê³  ìˆëŠ” ê³ ë¯¼ì€ ì´ë¯¸ ë‹¤ë¥¸ ì‚¬ëŒë“¤ë„ ë˜‘ê°™ì´ í–ˆì—ˆê³  ì´ë¯¸ í•´ê²°í•œ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ì ‘ ì§ˆë¬¸ì„ í•  ìˆ˜ë„ ìˆëŠ” ë§Œí¼, ìì‹ ì´ í•˜ê³  ìˆëŠ” ê³ ë¯¼ì´ êµ¬ê¸€ë§ìœ¼ë¡œë„ í•´ê²°í•˜ê¸° ì–´ë ¤ìš´ ì¼ì´ë¼ë©´ í•œë²ˆ í™•ì¸í•´ë³´ëŠ” ê²ƒë„ ë‚˜ì˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FClP0s0UN31o"
      },
      "source": [
        "## 3) Competition ì°¸ê°€ ğŸ¤\n",
        "\n",
        "**Active / Completed / InClass**\n",
        "\n",
        "í˜„ì¬ ì§„í–‰ ì¤‘ì´ë©°, ìƒê¸ˆì´ ê±¸ë ¤ ìˆëŠ” ì£¼ì œì™€ ë°ì´í„°ì…‹ì— ë¬´ì—‡ì´ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì§€ë‚œ ê²½ì—°ê³¼ ëŒ€í•™êµ ê°•ì˜ ë“± êµìœ¡ ëª©ì ìœ¼ë¡œ ê°œìµœí•œ ê²½ì—° ëŒ€íšŒ ëª©ë¡ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "**Team**\n",
        "\n",
        "ê·¸ëŸ¬ë‚˜, ì „ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ìê°€ í™•ì¸í•˜ê³  í™œìš©í•  ìˆ˜ ìˆëŠ” ì •ë³´ì— ëŒ€í•´ì„œëŠ” ì‚¬ì‹¤ìƒ í° ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì—¬ê¸°ëŠ” ì‹¤ì „!! íŒ€ì„ ë§Œë“¤ì–´ì„œ ê²½ì—° ëŒ€íšŒì— ì°¸ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (íŒŒí‹° í€˜ìŠ¤íŠ¸?)\n",
        "\n",
        "---\n",
        "\n",
        "**Leaderboard**\n",
        "\n",
        "Competitionì˜ ê½ƒ!! ì•„ë˜ì— ë³´ì´ëŠ” ë¦¬ë” ë³´ë“œëŠ” ì–´ëŠ ëª¨ë¸, ê·¸ë¦¬ê³  ì–´ë– í•œ íŒ€ì´ ê°€ì¥ ì˜ í–ˆëŠ”ì§€ ì§€ì •í•œ ê¸°ì¤€ì— ë”°ë¼ì„œ ìˆœìœ„ë¥¼ ë§¤ê¹ë‹ˆë‹¤. \n",
        "\n",
        "ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤... ë¨¸ì‹  ëŸ¬ë‹.. ëŒ€ë¶€ë¶„ì˜ ë°œì „ì€ ì´ëŸ¬í•œ ê²½ìŸì—ì„œ ë¹„ë¡¯ë˜ì—ˆë‹¤ê³  í•´ë„ ê³¼ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "**Submit**\n",
        "\n",
        "1ë²ˆ ë°©ë²•ìœ¼ë¡œ ë¡œì»¬ í™˜ê²½ì—ì„œ ì§ì ‘ ì‹¤í–‰í•œ ê²°ê³¼ë¥¼ ë‹´ì€ ì •ë‹µ csvë¥¼ ë§Œë“  í›„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "2ë²ˆ ë°©ë²•ìœ¼ë¡œëŠ” ì•ì„œ ì–¸ê¸‰í–ˆë˜ Kernels íƒ­ì—ì„œ, ì›¹ ìƒì—ì„œ ì‘ì„±í•˜ê³  ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì˜¨ë¼ì¸ì—ì„œ ë°”ë¡œ ì œì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sffc6heARZpo"
      },
      "source": [
        "## 4) Kaggleë¡œ Final Term í•´ê²°í•˜ê¸° ğŸ˜\n",
        "\n",
        "**Document Retrieval Question Anwsering**\n",
        "\n",
        "https://www.kaggle.com/c/bdc103-final\n",
        "\n",
        "ìœ„ì˜ ë§í¬ì—ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë°›ê³ ,\n",
        "\n",
        "- final_test_sample.json (ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” í•´ë‹¹ íŒŒì¼ë§Œ ì‚¬ìš©)\n",
        "- final_term_no_context.json\n",
        "\n",
        "ìœ„ì˜ ë‘ ê°œ íŒŒì¼ì„ ìì‹ ì˜ 'Google Drive' ìƒì˜ 'information_retrieval' í´ë”ì— ì˜®ê²¨ë³´ì.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBB818_2evDK"
      },
      "source": [
        "## ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤, ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ëŠ”ë°ê¹Œì§€ í•„ìš”í•œ ì¸ìë¥¼ ì‚¬ìš©ì ì •ì˜ ì¸ìë¡œ ì„¤ì •í•˜ëŠ” ë‚´ìš©ì„ ë‹´ê³  ìˆìŒ.\n",
        "## ì‚¬ìš©ë²•: python example.py --dataset_name squad.json .... \n",
        "## ì£¼í”¼í„° í™˜ê²½ì„ ìœ„í•œ ì¸ì ì„¤ì •ë²• \n",
        "import easydict\n",
        "import string\n",
        "import re\n",
        "def easydict_args():\n",
        "    \n",
        "    args = easydict.EasyDict({\n",
        "        \"dataset_name\": None,\n",
        "        \"dataset_config_name\": 'squad',\n",
        "        \"train_file\": 'final_test_no_context.json',\n",
        "        \"preprocessing_num_workers\": 4,\n",
        "        \"validation_file\": 'final_test_no_context.json',\n",
        "        \"test_file\": 'final_test_no_context.json',\n",
        "        \"test_save_path\" : \"result_final/final_test.csv\",\n",
        "        \"max_seq_length\": 384,\n",
        "        \"pad_to_max_length\": None,\n",
        "        \"model_name_or_path\": 'result/pytorch_model.bin',\n",
        "        \"config_name\": 'bert-base-cased',\n",
        "        \"tokenizer_name\": 'bert-base-cased',\n",
        "        \"use_slow_tokenizer\": None,\n",
        "        \"per_device_train_batch_size\": 4,\n",
        "        \"per_device_eval_batch_size\": 4,\n",
        "        \"learning_rate\": 3e-5,\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"max_train_steps\": None,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"lr_scheduler_type\": 'linear',\n",
        "        \"num_warmup_steps\":0,\n",
        "        \"output_dir\": 'result_final/',\n",
        "        \"seed\": 42,\n",
        "        \"doc_stride\": 128,\n",
        "        \"n_best_size\": 20,\n",
        "        \"null_score_diff_threshold\": 0.0,\n",
        "        \"version_2_with_negative\": False,\n",
        "        \"max_answer_length\": 30,\n",
        "        \"max_train_samples\": None,\n",
        "        \"max_eval_samples\": None,\n",
        "        \"overwrite_cache\": False,\n",
        "        \"max_predict_samples\": None,\n",
        "        \"do_train\": False,\n",
        "        \"do_eval\": True,\n",
        "        \"do_predict\": False,\n",
        "    })\n",
        "\n",
        "    ## ë¬´ê²°ì„± ì²´í¬\n",
        "    ## ì¸ìë¡œ ì „ë‹¬í•œ ë°ì´í„°ì…‹ ëª…ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ í™•ì¥ì ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° raise ì´í•˜ì˜ ë©”ì„¸ì§€ë¥¼ ì¶œë ¥\n",
        "    if (\n",
        "        args.dataset_name is None\n",
        "        and args.train_file is None\n",
        "        and args.validation_file is None\n",
        "        and args.test_file is None\n",
        "    ):\n",
        "        raise ValueError(\"Need either a dataset name or a training/validation/test file.\")\n",
        "    else:\n",
        "        if args.train_file is not None:\n",
        "            extension = args.train_file.split(\".\")[-1]\n",
        "            assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "        if args.validation_file is not None:\n",
        "            extension = args.validation_file.split(\".\")[-1]\n",
        "            assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if args.test_file is not None:\n",
        "            extension = args.test_file.split(\".\")[-1]\n",
        "            assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n",
        "\n",
        "    if args.output_dir is not None:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "    \n",
        "    ## ìœ„ì—ì„œ ì¸ìë¡œ ì „ë‹¬ë°›ì€ ê°’ì„ args ë³€ìˆ˜ì— ë‹´ì•„ ë°˜í™˜\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F_VCTNIhbpf"
      },
      "source": [
        "# SQuAD í‰ê°€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5QJ5tVbe009"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "def main():\n",
        "    args = easydict_args()\n",
        "\n",
        "    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
        "    ## ê°€ì†í™” ëª¨ë“œë¥¼ ì§€ì›í•˜ë©°, í•´ë‹¹ ëª¨ë“œë¡œ ë³‘ëª©ì´ ë°œìƒí•˜ëŠ” ë©”ì„œë“œ ë¶€ë¶„ì„ ëŒ€ì²´í•´ì„œ ì‚¬ìš©ê°€ëŠ¥.\n",
        "    accelerator = Accelerator()\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(accelerator.state)\n",
        "\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    # accelerator.is_local_main_process is only True for one process per machine.\n",
        "    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
        "    if accelerator.is_local_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # If passed along, set the training seed now.\n",
        "    if args.seed is not None:\n",
        "        set_seed(args.seed)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
        "    # 'text' is found. You can easily tweak this behavior (see below).\n",
        "    #\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    \n",
        "    ## Huggingfaceì˜ datasetsì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì…‹ ëª…ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ì €ì¥ëœ í˜•ì‹ì— ë§ì¶”ì–´ì„œ ë¡œë“œ (ìœ„ì˜ ë§í¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì°¸ì¡°)\n",
        "    if args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
        "    ## ì‚¬ìš©ì ì •ì˜ì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ì•„ë˜ì˜ ì¡°ê±´ë¬¸ ì‹¤í–‰. \n",
        "    else:\n",
        "        data_files = {}\n",
        "        if args.train_file is not None:\n",
        "            data_files[\"train\"] = args.train_file\n",
        "        if args.validation_file is not None:\n",
        "            data_files[\"validation\"] = args.validation_file\n",
        "        if args.test_file is not None:\n",
        "            data_files[\"test\"] = args.test_file\n",
        "        extension = args.train_file.split(\".\")[-1]\n",
        "        raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    ## ì§€ì •í•œ ëª¨ë¸ì˜ ì„¤ì •ì— ë§ì¶”ì–´ì„œ í™˜ê²½ êµ¬ì„± ìš”ì†Œ ë¡œë“œ\n",
        "    ## ex) BERT --> BERTConfig...\n",
        "    if args.config_name:\n",
        "        config = AutoConfig.from_pretrained(args.config_name)\n",
        "    elif args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "    \n",
        "    ## ì§€ì •í•œ ëª¨ë¸ì˜ ì„¤ì •ì— ë§ì¶”ì–´ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "    ## ex) BERT --> BERTTokenizer ... 30,000ê°œì˜ vocabì™€ word piece tokenzierë¥¼ ê³ë“¤ì¸...\n",
        "    if args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True)\n",
        "    elif args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    ## ì§€ì •í•œ ëª¨ë¸ ë¡œë“œí•˜ê¸°.\n",
        "    ## ì—¬ê¸°ì„œëŠ” ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ìƒë‹¨ì— ì§ˆì˜ì‘ë‹µ TASKë¥¼ ìœ„í•œ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ê³  ë¯¸ì„¸ì¡°ì •í›ˆë ¨ì„ í•˜ê¸°ìœ„í•´ ForQuestionAnsweringì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
        "    if args.model_name_or_path:\n",
        "        model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "            config=config,\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Training new model from scratch\")\n",
        "        model = AutoModelForQuestionAnswering.from_config(config)\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # Preprocessing is slighlty different for training and evaluation.\n",
        "    \n",
        "    ## ì „ì²˜ë¦¬í•˜ê¸° \n",
        "    column_names = raw_datasets[\"train\"].column_names\n",
        "    print(column_names)\n",
        "    #print(column_names)\n",
        "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
        "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
        "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
        "\n",
        "    # Padding side determines if we do (question|context) or (context|question).\n",
        "    pad_on_right = tokenizer.padding_side == \"right\"\n",
        "\n",
        "    ## ì¸ìë¡œ ì „ë‹¬í•œ ìµœëŒ€ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ì„ë² ë”© ê¸¸ì´ë³´ë‹¤ ê¸¸ ê²½ìš° ì—ëŸ¬ ë°˜í™˜\n",
        "    if args.max_seq_length > tokenizer.model_max_length:\n",
        "        logger.warning(\n",
        "            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
        "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
        "        )\n",
        "\n",
        "    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n",
        "\n",
        "    ##################\n",
        "\n",
        "    # Training preprocessing\n",
        "    ## í›ˆë ¨ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì „ì²˜ë¦¬ ì‘ì—…\n",
        "    ## íŒ¨ë“œì˜ ë°©í–¥ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì§€ë§Œ, ìµœê·¼ì—ëŠ” ë‹¤ë¥¸ ë°©í–¥ì—ë„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ê¸°ëŠ¥ ì¶”ê°€.\n",
        "    \n",
        "    def prepare_train_features(examples):\n",
        "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\n",
        "        # context that overlaps a bit the context of the previous feature.\n",
        "        tokenized_examples = tokenizer(\n",
        "            examples[question_column_name if pad_on_right else context_column_name],\n",
        "            examples[context_column_name if pad_on_right else question_column_name],\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "            max_length=max_seq_length,\n",
        "            stride=args.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\" if args.pad_to_max_length else False,\n",
        "        )\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "        # its corresponding example. This key gives us just that.\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "        # help us compute the start_positions and end_positions.\n",
        "        \n",
        "        ## spanì˜ ì—¬ë¶€ ì²´í¬\n",
        "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "        #print(\"offset_mapping\", offset_mapping)\n",
        "\n",
        "        # Let's label those examples!\n",
        "        ## span ê°€ëŠ¥í•˜ë„ë¡ ë³¸ë¬¸ì—ì„œ ì •ë‹µì´ ì¡´ì¬í•˜ëŠ” ì‹œì‘ì , ëì ì„ ë§¤í•‘í•˜ì—¬ ì €ì¥\n",
        "        tokenized_examples[\"start_positions\"] = []\n",
        "        tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            # We will label impossible answers with the index of the CLS token.\n",
        "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = examples[answer_column_name][sample_index]\n",
        "            # If no answers are given, set the cls_index as answer.\n",
        "            ## answer spanì´ ì—†ì„ ê²½ìš°ì—ëŠ” [CLS]ë¡œ ëŒ€ì²´\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Start/end character index of the answer in the text.\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                # Start token index of the current span in the text.\n",
        "                token_start_index = 0\n",
        "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                    token_start_index += 1\n",
        "\n",
        "                # End token index of the current span in the text.\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "                else:\n",
        "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                    while offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "        return tokenized_examples\n",
        "    \n",
        "    ######################\n",
        "    ## ì‚¬ìš©ì ì •ì˜ í›ˆë ¨ ë°ì´í„°ë¥¼ ì…ë ¥ ì¸ìë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, --do_trainì„ ì…ë ¥ ì¸ìë¡œ ë°˜ë“œì‹œ ì‚¬ìš©í•´ì•¼í•¨.\n",
        "    if \"train\" not in raw_datasets:\n",
        "        raise ValueError(\"--do_train requires a train dataset\")\n",
        "    train_dataset = raw_datasets[\"train\"]\n",
        "    ## ë””ë²„ê¹…ìš©\n",
        "    if args.max_train_samples is not None:\n",
        "        # We will select sample from whole data if agument is specified\n",
        "        train_dataset = train_dataset.select(range(args.max_train_samples))\n",
        "    \n",
        "    ## ë¡œë“œí•œ ì‚¬ìš©ì ì •ì˜ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ì…ë ¥ ë°ì´í„°ë¡œ ì¸ì½”ë”©\n",
        "    # Create train feature from dataset\n",
        "    train_dataset = train_dataset.map(\n",
        "        prepare_train_features,\n",
        "        batched=True,\n",
        "        num_proc=args.preprocessing_num_workers,\n",
        "        remove_columns=column_names,\n",
        "        load_from_cache_file=not args.overwrite_cache,\n",
        "    )\n",
        "    if args.max_train_samples is not None:\n",
        "        # Number of samples might increase during Feature Creation, We select only specified max samples\n",
        "        train_dataset = train_dataset.select(range(args.max_train_samples))\n",
        "\n",
        "    ######################\n",
        "    ## ìœ„ì˜ í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼\n",
        "    # Validation preprocessing\n",
        "    def prepare_validation_features(examples):\n",
        "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\n",
        "        # context that overlaps a bit the context of the previous feature.\n",
        "        tokenized_examples = tokenizer(\n",
        "            examples[question_column_name if pad_on_right else context_column_name],\n",
        "            examples[context_column_name if pad_on_right else question_column_name],\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "            max_length=max_seq_length,\n",
        "            stride=args.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\" if args.pad_to_max_length else False,\n",
        "        )\n",
        "\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "        # its corresponding example. This key gives us just that.\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
        "        # corresponding example_id and we will store the offset mappings.\n",
        "        tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "            context_index = 1 if pad_on_right else 0\n",
        "\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\n",
        "            sample_index = sample_mapping[i]\n",
        "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "            # position is part of the context or not.\n",
        "            tokenized_examples[\"offset_mapping\"][i] = [\n",
        "                (o if sequence_ids[k] == context_index else None)\n",
        "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "            ]\n",
        "\n",
        "        return tokenized_examples\n",
        "\n",
        "    #################\n",
        "    ## ì‚¬ìš©ì ì •ì˜ ê²€ì¦ ë°ì´í„°ë¥¼ ì…ë ¥ ì¸ìë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, --do_evalì„ ì…ë ¥ ì¸ìë¡œ ë°˜ë“œì‹œ ì‚¬ìš©í•´ì•¼í•¨.\n",
        "    if \"validation\" not in raw_datasets:\n",
        "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "    eval_examples = raw_datasets[\"validation\"]\n",
        "    if args.max_eval_samples is not None:\n",
        "        # We will select sample from whole data\n",
        "        eval_examples = eval_examples.select(range(args.max_eval_samples))\n",
        "    # Validation Feature Creation\n",
        "    eval_dataset = eval_examples.map(\n",
        "        prepare_validation_features,\n",
        "        batched=True,\n",
        "        num_proc=args.preprocessing_num_workers,\n",
        "        remove_columns=column_names,\n",
        "        load_from_cache_file=not args.overwrite_cache,\n",
        "    )\n",
        "\n",
        "    if args.max_eval_samples is not None:\n",
        "        # During Feature creation dataset samples might increase, we will select required samples again\n",
        "        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
        "\n",
        "    #################\n",
        "    ## ì‹¤ì œ ì •ë‹µì„ ë°˜í™˜í•˜ê³  ì‹¶ì„ ê²½ìš°\n",
        "    ## --do_predictì„ ì…ë ¥ ì¸ìë¡œ ì „ë‹¬í•˜ëŠ” ê²½ìš°, ì‚¬ìš©ì ì •ì˜ í‰ê°€ ë°ì´í„°ë¥¼ ì…ë ¥ ì¸ìë¡œ ì „ë‹¬í•´ì•¼í•¨. \n",
        "    if args.do_predict:\n",
        "        if \"test\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_examples = raw_datasets[\"test\"]\n",
        "        if args.max_predict_samples is not None:\n",
        "            # We will select sample from whole data\n",
        "            predict_examples = predict_examples.select(range(args.max_predict_samples))\n",
        "        # Predict Feature Creation\n",
        "        predict_dataset = predict_examples.map(\n",
        "            prepare_validation_features,\n",
        "            batched=True,\n",
        "            num_proc=args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not args.overwrite_cache,\n",
        "        )\n",
        "        if args.max_predict_samples is not None:\n",
        "            # During Feature creation dataset samples might increase, we will select required samples again\n",
        "            predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 3):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    #################\n",
        "    ## ì…ë ¥ ì¸ì½”ë”©ì„ ì™„ë£Œí•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ batch_size ë§Œí¼ ë¡œë“œí•˜ê¸° (pytorch í”„ë ˆì„ì›Œí¬ì˜ ì¥ì )\n",
        "    # DataLoaders creation:\n",
        "    if args.pad_to_max_length:\n",
        "        # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
        "        # to tensors.\n",
        "        data_collator = default_data_collator\n",
        "    else:\n",
        "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
        "        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
        "        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
        "        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n",
        "\n",
        "    ## í›ˆë ¨ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë¡œë” (ì§€ì •í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ë§Œ ë¡œë“œí•´ì„œ í›ˆë ¨í•˜ê¸°)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
        "    )\n",
        "\n",
        "    ## ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë¡œë”\n",
        "    eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
        "    )\n",
        "\n",
        "    ## ì‹¤ì œ í‰ê°€ë¥¼ ìœ„í•´ì„œ ì •ë‹µì˜ ê·¼ê±°ë¥¼ ì‚­ì œí•œ í˜•íƒœë¡œ inference\n",
        "    if args.do_predict:\n",
        "        predict_dataset_for_model = predict_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "        predict_dataloader = DataLoader(\n",
        "            predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
        "        )\n",
        "\n",
        "    ## huggingfaceì—ì„œ ì œê³µí•˜ëŠ” ê°„í¸í•˜ê²Œ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë„ë¡ í•˜ëŠ” í•¨ìˆ˜ \n",
        "    ## ê¹ƒí—™ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì€ utils_qa.pyë¥¼ ì°¸ì¡°\n",
        "    # Post-processing:\n",
        "    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
        "        # Post-processing: we match the start logits and end logits to answers in the original context.\n",
        "        predictions = postprocess_qa_predictions(\n",
        "            examples=examples,\n",
        "            features=features,\n",
        "            predictions=predictions,\n",
        "            version_2_with_negative=args.version_2_with_negative,\n",
        "            n_best_size=args.n_best_size,\n",
        "            max_answer_length=args.max_answer_length,\n",
        "            null_score_diff_threshold=args.null_score_diff_threshold,\n",
        "            output_dir=args.output_dir,\n",
        "            prefix=stage,\n",
        "        )\n",
        "        # Format the result to the format the metric expects.\n",
        "        if args.version_2_with_negative:\n",
        "            formatted_predictions = [\n",
        "                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
        "            ]\n",
        "        else:\n",
        "            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
        "            #formatted_predictions = [{\"id\": normalize_answer(k), \"prediction_text\": normalize_answer(v)} for k, v in predictions.items()]\n",
        "            df_pred = pd.DataFrame(formatted_predictions)\n",
        "            df_pred.to_csv(args.test_save_path , index=False, encoding='utf-8')\n",
        "\n",
        "        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
        "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
        "\n",
        "    ## F1 score, Exact match\n",
        "    metric = load_metric(\"squad_v2\" if args.version_2_with_negative else \"squad\")\n",
        "\n",
        "    #################\n",
        "    # Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
        "    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n",
        "        \"\"\"\n",
        "        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
        "        Args:\n",
        "            start_or_end_logits(:obj:`tensor`):\n",
        "                This is the output predictions of the model. We can only enter either start or end logits.\n",
        "            eval_dataset: Evaluation dataset\n",
        "            max_len(:obj:`int`):\n",
        "                The maximum length of the output tensor. ( See the model.eval() part for more details )\n",
        "        \"\"\"\n",
        "\n",
        "        step = 0\n",
        "        # create a numpy array and fill it with -100.\n",
        "        ## padì— í•´ë‹¹í•˜ëŠ” logit ê°’ì„ -100ìœ¼ë¡œ ì§€ì •\n",
        "        ## íš¨ìœ¨ì ì¸ ë°°ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì§€ì •í•œ max sequence lengthì— ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ì˜¬ë¦¬ëŠ” ë°©ë²•\n",
        "        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n",
        "        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather\n",
        "        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n",
        "            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n",
        "            # And after every iteration we have to change the step\n",
        "\n",
        "            batch_size = output_logit.shape[0]\n",
        "            cols = output_logit.shape[1]\n",
        "\n",
        "            if step + batch_size < len(dataset):\n",
        "                logits_concat[step : step + batch_size, :cols] = output_logit\n",
        "            else:\n",
        "                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n",
        "\n",
        "            step += batch_size\n",
        "\n",
        "        return logits_concat\n",
        "\n",
        "    # Optimizer\n",
        "    # Split weights in two groups, one with weight decay and the other not.\n",
        "    ## ìµœì í™” ê³¼ì •ì—ì„œ L2 Lossë¥¼ í†µí•´ì„œ ì§€ë‚˜ì¹œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ë¶€ë¶„. \n",
        "    ## L2 Regularization, ê¸°ì¡´ ì†ì‹¤ í•¨ìˆ˜ ê°’ì— ëª¨ë“  í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì œê³±í•˜ì—¬ í•©í•˜ê³  \n",
        "    ## ê¸°ì •í•œ ëŒë‹¤ ê°’ë§Œí¼ ì˜í–¥ë„ë¥¼ ì„¤ì •í•˜ëŠ”ë° ì´ë•Œ ëŒë‹¤ëŠ” ì •ê·œí™”ì˜ ì„¸ê¸°ë¥¼ ê²°ì •í•˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ \n",
        "    ## ì—¬ê¸°ì„œëŠ” weight_decayì— í•´ë‹¹\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    ## ì•„ë‹´ ìµœì í™” í•¨ìˆ˜ ì‚¬ìš©\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
        "\n",
        "    # Prepare everything with our `accelerator`.\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader\n",
        "    )\n",
        "\n",
        "    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
        "    # shorter in multiprocess)\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    ## ê°€ìƒì˜ ë¯¸ë‹ˆë°°ì¹˜ì‚¬ìš©. ìµœì¢… ì—…ë°ì´íŠ¸ëŠ” ê° ê°€ìƒ ë¯¸ë‹ˆ ë°°ì¹˜ì˜ gradient ê°’ì„ ëª¨ë‘ ê³ ë ¤í•œ global gradient\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if args.max_train_steps is None:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    else:\n",
        "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    ## schedulerë¥¼ í†µí•´ì„œ learning rateì˜ ì†ë„ ì¡°ì ˆ.\n",
        "    ## ì²˜ìŒì—ëŠ” í¬ê²Œ, ì ì  ì•½í•´ì§€ë„ë¡í•˜ë©° local minimumìœ¼ë¡œ ì¶”ì •ë˜ëŠ” êµ¬ê°„ì—ì„œëŠ” í¬ê²Œ ì§€ì •í•œ íšŸìˆ˜ ë§Œí¼ ì ìš© ê°€ëŠ¥.\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=args.lr_scheduler_type,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.num_warmup_steps,\n",
        "        num_training_steps=args.max_train_steps,\n",
        "    )\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    completed_steps = 0\n",
        "\n",
        "    for epoch in range(args.num_train_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "            accelerator.backward(loss)\n",
        "            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                progress_bar.update(1)\n",
        "                completed_steps += 1\n",
        "\n",
        "            if completed_steps >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "    # Validation\n",
        "    all_start_logits = []\n",
        "    all_end_logits = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            start_logits = outputs.start_logits # ëª¨ë“  í† í° ì¤‘ì—ì„œ ê°€ì¥ probabilityê°€ ë†’ì€ ì‹œì‘ì \n",
        "            end_logits = outputs.end_logits# ëª¨ë“  í† í° ì¤‘ì—ì„œ ê°€ì¥ probabilityê°€ ë†’ì€ ëì \n",
        "            \n",
        "            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
        "                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
        "                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
        "\n",
        "            all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n",
        "            all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n",
        "\n",
        "    max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
        "\n",
        "    # concatenate the numpy array\n",
        "    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n",
        "    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n",
        "\n",
        "    # delete the list of numpy arrays\n",
        "    del all_start_logits\n",
        "    del all_end_logits\n",
        "\n",
        "    outputs_numpy = (start_logits_concat, end_logits_concat)\n",
        "    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n",
        "    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
        "    logger.info(f\"Evaluation metrics: {eval_metric}\")\n",
        "\n",
        "    # Prediction\n",
        "    '''\n",
        "    if args.do_predict:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        all_start_logits = []\n",
        "        all_end_logits = []\n",
        "        for step, batch in enumerate(predict_dataloader):\n",
        "            batch = batch.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "                start_logits = outputs.start_logits\n",
        "                end_logits = outputs.end_logits\n",
        "\n",
        "                if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
        "                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
        "                    end_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
        "\n",
        "                all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n",
        "                all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n",
        "\n",
        "        max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
        "        # concatenate the numpy array\n",
        "        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n",
        "        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n",
        "\n",
        "        # delete the list of numpy arrays\n",
        "        del all_start_logits\n",
        "        del all_end_logits\n",
        "\n",
        "        outputs_numpy = (start_logits_concat, end_logits_concat)\n",
        "        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n",
        "        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
        "        logger.info(f\"Predict metrics: {predict_metric}\")\n",
        "    '''\n",
        "    #if args.output_dir is not None:\n",
        "        #accelerator.wait_for_everyone()\n",
        "        #unwrapped_model = accelerator.unwrap_model(model)\n",
        "        #unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An3Iy5vMMQ_G"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_gw9huEe6lz"
      },
      "source": [
        "# Tip!\n",
        "'''\n",
        "with open('final_test_no_context.json', 'r', encoding='utf-8') as f:\n",
        "    test_dataset = json.load(f)\n",
        "    for line, retrieved_line in zip(test_dataset['data'], retrieved_data):\n",
        "        line['context'] = retrieved_line # what you retrieved from document-based database\n",
        "\n",
        "with open('final_test_with_retrieved_text.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_dataset, f)\n",
        "    \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrHwIEYcMkL"
      },
      "source": [
        "## ğŸ‘ ì‹¤ìŠµ ì¢…ë£Œ!! ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤."
      ]
    }
  ]
}