{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습2강_정보검색.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Seo/BDC103_IR/blob/main/%EC%8B%A4%EC%8A%B52%EA%B0%95_%EC%A0%95%EB%B3%B4%EA%B2%80%EC%83%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx6X7EIQKF3e"
      },
      "source": [
        "## **실습 2.**\n",
        "\n",
        "본 실습 자료는 **BDC103(00) 빅데이터와정보검색 강의 실습**을 위해 **고려대학교 자연어처리연구실 (NLP & AI Lab)**에서 제작했습니다.\n",
        "\n",
        "☠️ 외부로의 무단 배포를 금지합니다. ☠️\n",
        "\n",
        "```\n",
        "version 1.0 (2021.03.04)\n",
        "created by: 서재형, 임희석 (고려대학교 자연어처리 연구실)\n",
        "email: wolhalang@gmail.com\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaK6IVShmEa3"
      },
      "source": [
        "## **정보 검색의 방법 😊**\n",
        "\n",
        "문서 검색에서 가장 기본적인 형태인 Boolean (0,1)로 구성.\n",
        "\n",
        "기존의 문서 단위 행렬 Document Term Matrix (DTM)은 단어의 출현 여부에 따라서 0과 1로 표현.\n",
        "\n",
        "**그러나, DTM은 어떤 문서가 더 중요한지, 더욱 일치햐는지 순위를 부여할 수 없음.**\n",
        "\n",
        "따라서, **단어 빈도수 Term-Frequency (TF)와 문서 역빈도수 Inverse Document Frquency (IDF)** 를 통해 문서 처리.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFqH3RJmqlqu"
      },
      "source": [
        "### **TF-IDF** \n",
        "\n",
        "> **Term Frequency (TF)**는 단어가 문서에 나타난 횟수를 의미하며, 해당 단어가 문서에 자주 등장할 수록 수치가 증가하여 높은 중요도를 나타낼 수 있다.\n",
        "\n",
        "> **Document Frequency (DF)**는 해당 단어가 나타난 문서의 수를 의미하며, 해당 단어가 다중 문서에 자주 등장할 수록 수치가 증가하여 낮은 중요도를 나타낼 수 있다.\n",
        "\n",
        "> **Inverse Document Frequency (IDF)**는 DF의 역수로 전체 단어 수를 해당 단어의 DF로 나눈 뒤 로그(log)를 취한 값이다. 즉 값이 클수록 중요해질 수 있도록 역수를 취한다 (TF와의 계산 편의성을 위해)\n",
        "\n",
        "> **TF-IDF는 TF와 IDF의 곱**으로 구성되며, 두 지표를 동시에 고려하는 가중치 산출 방법이다. \n",
        "\n",
        "\n",
        "$$tf(d,t)$$\n",
        "\n",
        "$$idf(d,t) = {log({n\\over 1+df(t)})}$$\n",
        "\n",
        "$$ w_d,_t = tf(d,t) * idf(d,t) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGNFrUrItkpy"
      },
      "source": [
        "### **TF-IDF 구현** 🤩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Qxmd8atohM"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meBFlkGdtzxP",
        "outputId": "2bf5810e-b941-4386-9d5c-39bb4f2b9b33"
      },
      "source": [
        "# 고려대학교 호상비문의 구절을 코퍼스로 해봅시다.\n",
        "epitaph = [\n",
        "  '민족의 힘으로 민족의 꿈을 가꾸어온',\n",
        "  '민족의 보람찬 대학이 있어',\n",
        "  '너 항상 여기에 자유의 불을 밝히고',\n",
        "  '정의의 길을 달리고 진리의 샘을 지키느니',\n",
        "  '지축을 박차고 포효하거라 너 불타는 야망',\n",
        "  '젊은 의욕의 상징아 우주를 향한 너의 부르짖음이',\n",
        "  '민족의 소리되어 메아리치는 곳에 너의 기개',\n",
        "  '너의 지조 너의 예지는',\n",
        "  '조국의 영원한 고동이 되리라'\n",
        "] \n",
        "\n",
        "vocab = []\n",
        "\n",
        "# 각 라인을 document로 생각해주세요.\n",
        "for line in epitaph:\n",
        "    for token in line.split():\n",
        "        vocab.append(token)\n",
        "\n",
        "print(vocab)\n",
        "\n",
        "vocab = list(set(vocab))\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['민족의', '힘으로', '민족의', '꿈을', '가꾸어온', '민족의', '보람찬', '대학이', '있어', '너', '항상', '여기에', '자유의', '불을', '밝히고', '정의의', '길을', '달리고', '진리의', '샘을', '지키느니', '지축을', '박차고', '포효하거라', '너', '불타는', '야망', '젊은', '의욕의', '상징아', '우주를', '향한', '너의', '부르짖음이', '민족의', '소리되어', '메아리치는', '곳에', '너의', '기개', '너의', '지조', '너의', '예지는', '조국의', '영원한', '고동이', '되리라']\n",
            "['포효하거라', '부르짖음이', '밝히고', '예지는', '불타는', '되리라', '야망', '지조', '민족의', '지축을', '항상', '고동이', '달리고', '너', '박차고', '향한', '자유의', '곳에', '있어', '상징아', '대학이', '여기에', '힘으로', '샘을', '조국의', '지키느니', '가꾸어온', '기개', '길을', '진리의', '메아리치는', '너의', '의욕의', '보람찬', '젊은', '영원한', '우주를', '정의의', '소리되어', '불을', '꿈을']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIUwkRfCvmiM"
      },
      "source": [
        "# TF, IDF 구현 함수\n",
        "def tf(t,d):\n",
        "    return d.count(t)\n",
        "\n",
        "def idf(t,document):\n",
        "    df_count = 0\n",
        "    for doc in document:\n",
        "        if t in doc:\n",
        "            df_count += 1\n",
        "    return np.log(len(epitaph)/(df_count+1))\n",
        "\n",
        "def tfidf(t,d):\n",
        "    return tf(t,d)*idf(t,epitaph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "6fcB3de7wjB3",
        "outputId": "cf715ad8-1ab7-4f30-aeca-8dadf1715350"
      },
      "source": [
        "# TF-IDF 구현\n",
        "result = []\n",
        "for i in range(len(epitaph)):\n",
        "    result.append([])\n",
        "    d = epitaph[i]\n",
        "    for j in range(len(vocab)):\n",
        "        t = vocab[j]\n",
        "        result[-1].append(tfidf(t,d))\n",
        "\n",
        "tf_idf = pd.DataFrame(result, columns = vocab)\n",
        "tf_idf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>포효하거라</th>\n",
              "      <th>부르짖음이</th>\n",
              "      <th>밝히고</th>\n",
              "      <th>예지는</th>\n",
              "      <th>불타는</th>\n",
              "      <th>되리라</th>\n",
              "      <th>야망</th>\n",
              "      <th>지조</th>\n",
              "      <th>민족의</th>\n",
              "      <th>지축을</th>\n",
              "      <th>항상</th>\n",
              "      <th>고동이</th>\n",
              "      <th>달리고</th>\n",
              "      <th>너</th>\n",
              "      <th>박차고</th>\n",
              "      <th>향한</th>\n",
              "      <th>자유의</th>\n",
              "      <th>곳에</th>\n",
              "      <th>있어</th>\n",
              "      <th>상징아</th>\n",
              "      <th>대학이</th>\n",
              "      <th>여기에</th>\n",
              "      <th>힘으로</th>\n",
              "      <th>샘을</th>\n",
              "      <th>조국의</th>\n",
              "      <th>지키느니</th>\n",
              "      <th>가꾸어온</th>\n",
              "      <th>기개</th>\n",
              "      <th>길을</th>\n",
              "      <th>진리의</th>\n",
              "      <th>메아리치는</th>\n",
              "      <th>너의</th>\n",
              "      <th>의욕의</th>\n",
              "      <th>보람찬</th>\n",
              "      <th>젊은</th>\n",
              "      <th>영원한</th>\n",
              "      <th>우주를</th>\n",
              "      <th>정의의</th>\n",
              "      <th>소리되어</th>\n",
              "      <th>불을</th>\n",
              "      <th>꿈을</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.62186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.81093</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.810930</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.62186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      포효하거라     부르짖음이       밝히고  ...      소리되어        불을        꿈을\n",
              "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.504077\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2  0.000000  0.000000  1.504077  ...  0.000000  1.504077  0.000000\n",
              "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "4  1.504077  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "5  0.000000  1.504077  0.000000  ...  0.000000  0.000000  0.000000\n",
              "6  0.000000  0.000000  0.000000  ...  1.504077  0.000000  0.000000\n",
              "7  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "8  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[9 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3IbphCzOTu"
      },
      "source": [
        "### **BM25** \n",
        "\n",
        "➀ $f(q_i, D)$: 문서에 존재하는 키워드 갯수\n",
        "\n",
        "➁ $k_1$: 임의의 상수, default 1.2\n",
        "\n",
        "➂ $b$: 임의의 상수, default 0.75\n",
        "\n",
        "➃ $|D|$: 현재 검색된 문서 길이\n",
        "\n",
        "➄ $avgDL$: 전체 문서의 평균적인 길이\n",
        "\n",
        "\n",
        "> `score(Document D, Query q)`에 대한 BM25 수식\n",
        "\n",
        "$$ \\sum_{i=1}^{n}IDF(q_i)\\cdot\\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{avgDL})} $$\n",
        "\n",
        "\n",
        "> BM25는 TF-IDF와 같은 스코어 계산 방식이지만 **색인화된 평균 문서의 길이**와 **검색된 문서의 길이를 고려**했다고 보면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DG_dw8vPzBp"
      },
      "source": [
        "### **BM25 구현** 🤩\r\n",
        "\r\n",
        "> python의 경우 `rank-bm25` 패키지를 통해서 BM25를 간편하게 사용할 수 있습니다.\r\n",
        "\r\n",
        "```\r\n",
        "pip install rank_bm25\r\n",
        "```\r\n",
        "\r\n",
        "> 패키지 참조\r\n",
        "\r\n",
        "https://pypi.org/project/rank-bm25/\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUUeZ66ABhF_",
        "outputId": "182d26ee-8df9-4284-f11c-4aeac75b4588"
      },
      "source": [
        "!pip install rank_bm25"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LLZWkMWECXU"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\r\n",
        "\r\n",
        "## 응원가 제목을 코퍼스화\r\n",
        "songs = [\r\n",
        "  '민족의 아리아',\r\n",
        "  '젊은 그대',\r\n",
        "  '들어라 보아라 기억하라',\r\n",
        "  '뱃노래',\r\n",
        "  '석탑',\r\n",
        "  '엘리제를 위하여',\r\n",
        "  '고래사냥',\r\n",
        "  '지야의 함성',\r\n",
        "  '레이몽드 서곡'\r\n",
        "]\r\n",
        "\r\n",
        "## 코퍼스에 대해서 BM25 알고리즘 적용을 위해 토큰화하여 색인화\r\n",
        "tokenized_songs = [song.split(\" \") for song in songs]\r\n",
        "bm25 = BM25Okapi(tokenized_songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaAsk4kmFo_z",
        "outputId": "fc072bda-eddf-48fe-fbd5-a9e2b2d009d1"
      },
      "source": [
        "# 질의를 만들어보고 색인화된 문서에 대한 doc_scores를 반환\r\n",
        "query = \"민족의 노래\"\r\n",
        "tokenized_query = query.split(\" \")\r\n",
        "doc_scores = bm25.get_scores(tokenized_query)\r\n",
        "print(doc_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.64222585 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PqseGiMGJ-O",
        "outputId": "e12ccc96-4a17-44cc-fe5f-38828016a0bd"
      },
      "source": [
        "# 가장 높은 점수의 결과 반환\r\n",
        "bm25.get_top_n(tokenized_query, songs, n=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['민족의 아리아']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjzBPl_lNJZj"
      },
      "source": [
        "<img src = 'https://miro.medium.com/max/1200/1*zvPL19PUTMslrhXPODj_Og.png' width = 300>\r\n",
        "\r\n",
        "다음은... **BM25 알고리즘**을 기본 방법론으로 사용하는 \r\n",
        "\r\n",
        "**ElasticSearch 검색 엔진**을 **Python**에서 활용하는 방법에 대해서 배우도록 하겠습니다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR5VDJ1HJfZp"
      },
      "source": [
        "## **ElasticSearch를 활용한 정보 검색** 🧐\r\n",
        "\r\n",
        "공식 홈페이지 참조: https://www.elastic.co/kr/what-is/elasticsearch\r\n",
        "\r\n",
        "#### **ElasticSearch란?**\r\n",
        "\r\n",
        "> Elasticsearch는 **텍스트, 숫자, 위치 기반 정보, 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진으로 분산형 및 개방형을 특징**으로 합니다. Elasticsearch는 Apache Lucene을 기반으로 구축되었으며, Elasticsearch N.V.(현재 명칭 Elastic)가 2010년에 최초로 출시했습니다. 간단한 REST API, 분산형 특징, 속도, 확장성으로 유명한 Elasticsearch는 데이터 수집, 보강, 저장, 분석, 시각화를 위한 무료 개방형 도구 모음인 Elastic Stack의 핵심 구성 요소입니다.\r\n",
        "\r\n",
        "#### **ElasticSearch에서 색인이란?**\r\n",
        "\r\n",
        "> Elasticsearch **인덱스는 서로 관련되어 있는 문서들의 모음**입니다. Elasticsearch는 **JSON 문서로 데이터를 저장**합니다. 각 문서는 **일련의 키**(필드나 속성의 이름)와 **그에 해당하는 값**(문자열, 숫자, 부울, 날짜, 값의 배열, 지리적 위치 또는 기타 데이터 유형)을 서로 연결합니다.\r\n",
        "\r\n",
        "> Elasticsearch는 **역 인덱스**라고 하는 데이터 구조를 사용하는데, 이것은 아주 빠른 풀텍스트 검색을 할 수 있도록 설계된 것입니다. 역 인덱스는 **문서에 나타나는 모든 고유한 단어의 목록을 만들고, 각 단어가 발생하는 모든 문서를 식별**합니다.\r\n",
        "\r\n",
        "> 색인 프로세스 중에, **Elasticsearch는 문서를 저장하고 역 인덱스를 구축하여 거의 실시간으로 문서를 검색** 가능한 데이터로 만듭니다. 인덱스 API를 사용해 색인이 시작되며, 이를 통해 사용자는 특정한 인덱스에서 JSON 문서를 추가하거나 업데이트할 수 있습니다.\r\n",
        "\r\n",
        "#### **사용하는 이유?**\r\n",
        "\r\n",
        "> Elasticsearch는 Lucene을 기반으로 구축되기 때문에, **풀텍스트 검색에 뛰어납니다**. Elasticsearch는 또한 거의 **실시간 검색 플랫폼**입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다.\r\n",
        "\r\n",
        "> 분산 시스템으로 **병렬적인 처리. 검색 대상의 사이즈가 아주 크더라도 분석 및 처리가 가능**합니다.\r\n",
        "\r\n",
        "**--> 빅데이터 검색에 적합한 검색 엔진**\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJ97wRkZIZr"
      },
      "source": [
        "### **구글 코랩과 구글 드라이브 연동하기** 😎\n",
        "\n",
        "구글 코랩 환경에서는 로그인되어 있는 구글 계정과 연결되어 있는 '구글 드라이브'와의 연동을 지원합니다.  \n",
        "연동하는 경우, 자신의 구글 드라이브를 로컬 환경처럼 활용이 가능하며, 원하는 파일이나 이미지를 불러올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZGi15R3ZBNE"
      },
      "source": [
        "### Google Drive 패키지와 os 모듈 불러오기\n",
        "from google.colab import drive\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QESTJMWZJgs",
        "outputId": "0f942317-7598-4366-f277-e459ed9983dd"
      },
      "source": [
        "### 본인 구글 드라이브의 최초 경로를 설정하기\n",
        "\n",
        "# 대부분의 구글 드라이브 최초 경로는 아래와 같습니다. \n",
        "# 예외 발생 시 본인의 구글 드라이브에 접속하여 content 폴더나 gdrive 폴더가 어떤 위치에 있으며, \n",
        "# 자신이 연결하려는 폴더까지의 경로가 어떻게 되는지 확인해야 합니다.  \n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BGTF2zT6ZMp6",
        "outputId": "c15171c4-7e45-4a39-88f5-90c7d77c0b12"
      },
      "source": [
        "### 자신의 현재 경로를 파악하기\n",
        "\n",
        "os.getcwd() ## 아마도 /content로 나올 것."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRCwpRm1ije"
      },
      "source": [
        "### **엘라스틱 서버 설치 및 구동하기** 😎"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_DmUR_krAJN"
      },
      "source": [
        "### 구글 클라우드 컴퓨터에 elastic server 서버 설치를 위한 폴더 생성 \n",
        "!sudo mkdir /content/elasticsearch\n",
        "### 접근 권한 수정\n",
        "!chmod 755 -R elasticsearch\n",
        "### 현재 작업 디렉토리 설정\n",
        "os.chdir('/content/elasticsearch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CcNnHCosV4Y"
      },
      "source": [
        "### 리눅스용 엘라스틱서치 서버 설치를 위한 패키지 다운로드\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4UVAmFWsd_Z"
      },
      "source": [
        "### 위에서 다운로드 받은 압축 파일을 해제\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfUtZwAHskoH"
      },
      "source": [
        "### 코랩 노트북 환경에서 서버 구동을 위해서 PPID 1의 백그라운드 데몬 프로세스가 해당 폴더에 접근이 가능하도록 소유자 변경\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jjugup6tEF6",
        "outputId": "eb1c542a-c70f-4d1d-b9c2-beee051d3757"
      },
      "source": [
        "### 파이썬 환경에서 구동을 위한 elasticsearch 패키지 설치\n",
        "!pip install elasticsearch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/68/76c5d46cc6a48fddb759f585bc8728caa11bfc9b812ce6705fc5f99beab2/elasticsearch-7.11.0-py2.py3-none-any.whl (325kB)\n",
            "\r\u001b[K     |█                               | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 21.4MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 174kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 204kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 215kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 225kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 235kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 245kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 256kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 266kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 276kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 286kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 296kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 307kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 317kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 327kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2020.12.5)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDzbifJStJmw"
      },
      "source": [
        "# 데몬 프로세스로 엘라스틱 서버 개시하기\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es = Popen(['elasticsearch-7.0.0/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zg-u5fntcFT"
      },
      "source": [
        "### **3분 정도 기다리시면 됩니다!!!** 😄\n",
        "\n",
        "서버가 안정적으로 열릴 때까지 시간이 조금 걸릴 수 있어요..\n",
        "\n",
        "`es.info()`를 실행시켰을 때\n",
        "\n",
        "아래와 같은 결과가 나오면 성공!\n",
        "\n",
        "```\n",
        "{'cluster_name': 'elasticsearch',\n",
        " 'cluster_uuid': 'EOh60gEwRri9A_UruHyQLA',\n",
        " 'name': '657b2f379e3e',\n",
        " 'tagline': 'You Know, for Search',\n",
        " 'version': {'build_date': '2019-04-05T22:55:32.697037Z',\n",
        "  'build_flavor': 'default',\n",
        "  'build_hash': 'b7e28a7',\n",
        "  'build_snapshot': False,\n",
        "  'build_type': 'tar',\n",
        "  'lucene_version': '8.0.0',\n",
        "  'minimum_index_compatibility_version': '6.0.0-beta1',\n",
        "  'minimum_wire_compatibility_version': '6.7.0',\n",
        "  'number': '7.0.0'}}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZD2PfkXt9Xy",
        "outputId": "8a74066d-b8b3-471b-f2fd-d763db884d29"
      },
      "source": [
        "# 로컬 서버에 엘라스틱 서버와 python을 연결\n",
        "from elasticsearch import Elasticsearch\n",
        "es = Elasticsearch(\"localhost:9200/\")\n",
        "es.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cluster_name': 'elasticsearch',\n",
              " 'cluster_uuid': 'GgL6IF_NRlOFMC9cIxTEUw',\n",
              " 'name': 'ef098ec87dba',\n",
              " 'tagline': 'You Know, for Search',\n",
              " 'version': {'build_date': '2019-04-05T22:55:32.697037Z',\n",
              "  'build_flavor': 'default',\n",
              "  'build_hash': 'b7e28a7',\n",
              "  'build_snapshot': False,\n",
              "  'build_type': 'tar',\n",
              "  'lucene_version': '8.0.0',\n",
              "  'minimum_index_compatibility_version': '6.0.0-beta1',\n",
              "  'minimum_wire_compatibility_version': '6.7.0',\n",
              "  'number': '7.0.0'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqTiE1R6vjh9"
      },
      "source": [
        "### **Elasticsearch Server 개시 및 Python과 연동**\n",
        "\n",
        "> /content/gdrive 이후의 경로에는 구글 드라이브 보안으로 인해서\n",
        "daemon process가 정상적인 방식으로는 서버 구동이 어렵기 때문에 본인 드라이브가 아닌\n",
        "구글 클라우드 컴퓨터에 엘라스틱 서버를 설치하고 개시했습니다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsuXyBkVZPxA"
      },
      "source": [
        "### 현재 작업 환경 재설정하기 (자신만의 작업 디렉토리를 기본 경로로 설정하는 것)\n",
        "\n",
        "#.ipynb 노트북 파일과 불러오려는 이미지 및 모듈에 해당하는 파일들은 같은 현재 작업 환경 \n",
        "#또는 그 하위 폴더에 존재하는 것이 좋습니다. \n",
        "\n",
        "# 자신만의 기본 경로를 설정하는데, 대부분의 경우에는 Colab Notebooks까지 동일하게 공유하며,\n",
        "# 이후 경로는 자신이 생성한 폴더에 맞추어서 경로를 설정하면 됩니다.\n",
        "# 수업에서는 자신의 구글 드라이브에 information_retrieval 폴더를 생성하시면 됩니다. \n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TOmX0hPZdS0"
      },
      "source": [
        "### 🤥💦 모르는 것이 있을 경우에는?\n",
        "\n",
        "프로그래밍의 첫 걸음은 **`검색`** 입니다.\n",
        "\n",
        "특히 **`빅데이터와 정보 검색`** 수업을 듣고 있는 여러분들은 **반.드.시** 궁금하거나 모르는 내용에 대해서 **`검색`**하는 습관이 필요합니다.\n",
        "\n",
        "> Google 검색 엔진 \n",
        "                \n",
        "    -> \"Python 루프문\"\n",
        "                 \n",
        "    -> \"Python for문 에러\"\n",
        "\n",
        "    -> \"What is SyntaxError: invalid syntax in python?\"\n",
        "\n",
        "등의 `에러문장`과 `키워드`를 적극 활용해주세요!😄\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDsLe1QI1QDo"
      },
      "source": [
        "### **데이터에 대해서 새롭게 인덱싱하기** 😄\n",
        "\n",
        "> 딕셔너리 구조의 데이터로 간단하게 연습해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlUp5Tsw1NhC",
        "outputId": "c5a9a297-3235-4081-e809-ecb0d34a95da"
      },
      "source": [
        "# 데이터를 색인화하기 위한 함수\n",
        "def indexing(es, index_name):\n",
        "    # 이미 존재할 경우 삭제하고 다시 만들기\n",
        "    if es.indices.exists(index=index_name):\n",
        "        es.indices.delete(index=index_name)\n",
        "\n",
        "    # 인덱스 생성\n",
        "    print(es.indices.create(index=index_name))\n",
        "\n",
        "# 인덱스명을 정하기 (자유롭게)\n",
        "index_name = 'sample_index'\n",
        "indexing(es, index_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'sample_index'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68zDcAB6-rkF"
      },
      "source": [
        "### **엘라스틱서치에서 색인화는 어떻게 할까요?** 😊\n",
        "\n",
        "> 변수명, key, value에 해당하는 부분은 자유롭게 작성하면 됩니다. 딕셔너리 자료형을 기본으로 하기 때문에, 해당 형식에 맞추어서 만들 수 있습니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f96zbclC_kG7",
        "outputId": "f949dbae-25de-40f9-cb18-b631c764784f"
      },
      "source": [
        "### 새로운 데이터 인덱싱 \n",
        "## 여기서는 '노래 제목'과 '가사 일부'를 mapping했습니다.\n",
        "sample1= {'songs': '민족의 아리아', 'lyrics': '지축을 박차고 포효하라 그대'}\n",
        "sample2= {'songs': '뱃노래', 'lyrics': '즐거운 고연전 날에 지고 가는 연대생이 처량도 하구나'}\n",
        "sample3= {'songs': '석탑', 'lyrics': '이름모를 석공의 땀과 눈물이 흘러내리네'}\n",
        "sample4= {'songs': 'Forever', 'lyrics': '우리의 함성은 신화가 되리라 울려라 이곳에 Forever'}\n",
        "sample5= {'songs': '젊은그대', 'lyrics': '사랑스런 젊은 그대 태양같은 젊은 그대'}\n",
        "\n",
        "# es 변수에 객체화한 서버는 위에서 정의한 데아터에 대해서\n",
        "# 이전 셀에서 선언한 'sample_index'로 인덱스명을 정하고\n",
        "# 데이터의 타입은 문자열인 상태로 색인화를 진행합니다.\n",
        "es.index(index=index_name, doc_type='string', body = sample1)\n",
        "es.index(index=index_name, doc_type='string', body = sample2)\n",
        "es.index(index=index_name, doc_type='string', body = sample3)\n",
        "es.index(index=index_name, doc_type='string', body = sample4)\n",
        "es.index(index=index_name, doc_type='string', body = sample5)\n",
        "\n",
        "es.indices.refresh(index=index_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
            "  warnings.warn(message, category=ElasticsearchWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_shards': {'failed': 0, 'successful': 1, 'total': 2}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17hSJcKEAJY7"
      },
      "source": [
        "### **색인화한 검색 엔진에서 정보 검색해보기** 🤩\n",
        "\n",
        "> es.search를 활용해서 검색해보기\n",
        "\n",
        "```\n",
        "es.search(index = '선언한 인덱스명', body = {'from': '몇 위부터 반환할 것인지' , \n",
        "'size': '최대 반환할 갯수', 'query' : {'match':{'기준 key' : '검색할 내용'}}})\n",
        "```\n",
        "\n",
        "### 결과로는 무엇이 반환되었을까...? 🧐\n",
        "\n",
        "> 딕셔너리 형태로, 필요한 정보는 딕셔너리 value에 대한 접근 방식을 통해 얻을 수 있습니다.\n",
        "\n",
        "```\n",
        "{'took': 3, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, \n",
        "'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 2, 'relation': 'eq'},\n",
        "'max_score': 1.2037696, 'hits': [{'_index': 'sample_index', '_type': 'string',\n",
        "'_id': 'zYMJ9HcB0qdz_4P6yRNV', '_score': 1.2037696, '_source': \n",
        "{'songs': '젊은그대', 'lyrics': '사랑스런 젊은 그대 태양같은 젊은 그대'}}, \n",
        "{'_index': 'sample_index', '_type': 'string', '_id': 'yYMJ9HcB0qdz_4P6yBPd',\n",
        "'_score': 1.0137007, '_source': {'songs': '민족의 아리아', 'lyrics': '지축을 박차고 \n",
        "포효하라 그대'}}]}}\n",
        "```\n",
        "\n",
        "> 딕셔너리의 접근은..?\n",
        "\n",
        "```\n",
        "딕셔너리 변수명['key'] # value를 반환! \n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3r0EBhuEFgy",
        "outputId": "16a213a2-3c99-4869-d5ea-8d58891cb0ae"
      },
      "source": [
        "# 검색 해보기\n",
        "results = es.search(index=index_name, body={'from':0, 'size':10, 'query':{'match':{'lyrics':'그대'}}})\n",
        "\n",
        "# 딕셔너리 안에 딕셔너리: 'hits'라는 key 안에 value는 또 다른 사전형으로 되어있으며, 해당 사전의 key 이름은 'hits'   \n",
        "for result in results['hits']['hits']:\n",
        "    print('score:', result['_score'], 'source::', result['_source'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score: 1.2037696 source:: {'songs': '젊은그대', 'lyrics': '사랑스런 젊은 그대 태양같은 젊은 그대'}\n",
            "score: 1.0137007 source:: {'songs': '민족의 아리아', 'lyrics': '지축을 박차고 포효하라 그대'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVT31VFMFxFF"
      },
      "source": [
        "### **위키 데이터를 사용해서 인덱싱하기** ☺️\n",
        "\n",
        "> 영어 나무위키 덤프를 다운로드 받은 다음에 전처리하여 사용해보기\n",
        "\n",
        "https://dumps.wikimedia.org/enwiki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aDqvMLNg7jC",
        "outputId": "286989f5-3e2a-4c46-e163-9fd9830312c0"
      },
      "source": [
        "# 샘플 영어 위키 덤프를 다운로드\n",
        "!wget https://github.com/AlonEirew/wikipedia-to-elastic/raw/master/dumps/tinywiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-03 05:16:07--  https://github.com/AlonEirew/wikipedia-to-elastic/raw/master/dumps/tinywiki-latest-pages-articles.xml.bz2\n",
            "Resolving github.com (github.com)... 13.114.40.48\n",
            "Connecting to github.com (github.com)|13.114.40.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AlonEirew/wikipedia-to-elastic/master/dumps/tinywiki-latest-pages-articles.xml.bz2 [following]\n",
            "--2021-03-03 05:16:07--  https://raw.githubusercontent.com/AlonEirew/wikipedia-to-elastic/master/dumps/tinywiki-latest-pages-articles.xml.bz2\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9731 (9.5K) [application/octet-stream]\n",
            "Saving to: ‘tinywiki-latest-pages-articles.xml.bz2’\n",
            "\n",
            "tinywiki-latest-pag 100%[===================>]   9.50K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-03-03 05:16:07 (7.91 MB/s) - ‘tinywiki-latest-pages-articles.xml.bz2’ saved [9731/9731]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWTWx1B18DU0"
      },
      "source": [
        "### **WikiExtractor를 사용해서 영어 위키데이터 전처리** 👍\n",
        "\n",
        "<img src = \"https://avatars.githubusercontent.com/u/356002?s=460&v=4\" width = \"300\" >\n",
        "\n",
        "> WikiExtractor https://github.com/attardi/wikiextractor\n",
        "\n",
        "```\n",
        "@misc{Wikiextractor2015,\n",
        "  author = {Giusepppe Attardi},\n",
        "  title = {WikiExtractor},\n",
        "  year = {2015},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{https://github.com/attardi/wikiextractor}}\n",
        "}\n",
        "```\n",
        "\n",
        "위키 데이터에 포함되어 있는 불필요한 태그, 특수 문자, 기호, 숫자 등을 제거하고\n",
        "\n",
        "원하는 부분을 별도로 추출이 가능 ex) 문서 본문, 제목, 저자 등\n",
        "\n",
        "실습에서는 문서의 제목과 본문 위주로 파싱을 진행.\n",
        "\n",
        "*더 자세한 내용은 위의 저장소 주소를 통해서 확인하실 수 있습니다.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jijQiwAv2oaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1018e635-1c51-48d0-c089-196da8a8ac35"
      },
      "source": [
        "# 위키데이터의 노이즈를 제거하고 json 형태로 반환하는 코드를 참조\n",
        "!git clone https://github.com/attardi/wikiextractor.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wikiextractor'...\n",
            "remote: Enumerating objects: 733, done.\u001b[K\n",
            "remote: Total 733 (delta 0), reused 0 (delta 0), pack-reused 733\u001b[K\n",
            "Receiving objects: 100% (733/733), 1.28 MiB | 4.75 MiB/s, done.\n",
            "Resolving deltas: 100% (427/427), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfm7Hrs6xOaT",
        "outputId": "15fce260-5471-4090-9fc8-634479758675"
      },
      "source": [
        "# 다운로드 받은 샘플 위키 데이터를 전처리하여 검색의 입력으로 사용\n",
        "# 결과는 elastic 폴더에 'extract_result/AA'라는 새로운 폴더에 저장된다.\n",
        "!python -m wikiextractor.wikiextractor.WikiExtractor tinywiki-latest-pages-articles.xml.bz2 --json -o extract_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Preprocessing 'tinywiki-latest-pages-articles.xml.bz2' to collect template definitions: this may take some time.\n",
            "INFO: Loaded 0 templates in 0.0s\n",
            "INFO: Starting page extraction from tinywiki-latest-pages-articles.xml.bz2.\n",
            "INFO: Using 1 extract processes.\n",
            "INFO: Finished 1-process extraction of 7 articles in 0.0s (159.5 art/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WInyOqC7-WI"
      },
      "source": [
        "### **전처리한 위키데이터로 검색 시스템 만들기** 🧐\n",
        "\n",
        "<img src = \"https://github.com/facebookresearch/DrQA/raw/master/img/drqa.png\" width = \"600\" >\n",
        "\n",
        "> This is a PyTorch implementation of the DrQA system described in the ACL 2017 paper Reading Wikipedia to Answer Open-Domain Questions.\n",
        "\n",
        "> ACL 2017. 페이스북에서 공개한 정보 검색 기반 질의 응답 시스템입니다. \n",
        "\n",
        "> 논문 링크: https://arxiv.org/abs/1704.00051\n",
        "\n",
        "본 실습은 위의 논문에서 제시하는 방식을 기반으로 진행됩니다.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@inproceedings{chen2017reading,\n",
        "  title={Reading {Wikipedia} to Answer Open-Domain Questions},\n",
        "  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},\n",
        "  booktitle={Association for Computational Linguistics (ACL)},\n",
        "  year={2017}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOnyt1naEoug",
        "outputId": "31ba4c5e-d648-442e-e5a5-ea6905a3632f"
      },
      "source": [
        "# 엘라스틱서치 서버와 마찬가지로 구글 드라이브 내부 보안 정책으로 인해,\n",
        "# DRQA의 필수 설치 패키지인 CoreNLP 서버 연동을 위해서 구글 드라이브 바깥의 구글 클라우드 컴퓨터에 저장소 코드를 다운 받습니다.\n",
        "os.chdir('/content/elasticsearch/')\n",
        "!git clone https://github.com/facebookresearch/DrQA.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DrQA'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Total 265 (delta 0), reused 0 (delta 0), pack-reused 265\u001b[K\n",
            "Receiving objects: 100% (265/265), 562.37 KiB | 1.45 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3se7ZZC2E2A0",
        "outputId": "1b0dd307-95c0-47da-8ac1-5a201522a99d"
      },
      "source": [
        "# 내려받은 저장소 패키지 안에 필수 항목 설치 \n",
        "os.chdir('/content/elasticsearch/DrQA')\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (7.11.0)\n",
            "Requirement already satisfied: pexpect==4.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->-r requirements.txt (line 6)) (0.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from prettytable->-r requirements.txt (line 6)) (54.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r requirements.txt (line 9)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect==4.2.1->-r requirements.txt (line 10)) (0.7.0)\n",
            "running develop\n",
            "running egg_info\n",
            "writing drqa.egg-info/PKG-INFO\n",
            "writing dependency_links to drqa.egg-info/dependency_links.txt\n",
            "writing requirements to drqa.egg-info/requires.txt\n",
            "writing top-level names to drqa.egg-info/top_level.txt\n",
            "writing manifest file 'drqa.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.7/dist-packages/drqa.egg-link (link to .)\n",
            "drqa 0.1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /content/elasticsearch/DrQA\n",
            "Processing dependencies for drqa==0.1.0\n",
            "Searching for pexpect==4.2.1\n",
            "Best match: pexpect 4.2.1\n",
            "Adding pexpect 4.2.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for elasticsearch==7.11.0\n",
            "Best match: elasticsearch 7.11.0\n",
            "Adding elasticsearch 7.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for nltk==3.2.5\n",
            "Best match: nltk 3.2.5\n",
            "Adding nltk 3.2.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for prettytable==2.0.0\n",
            "Best match: prettytable 2.0.0\n",
            "Adding prettytable 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tqdm==4.41.1\n",
            "Best match: tqdm 4.41.1\n",
            "Adding tqdm 4.41.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for regex==2019.12.20\n",
            "Best match: regex 2019.12.20\n",
            "Adding regex 2019.12.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for ptyprocess==0.7.0\n",
            "Best match: ptyprocess 0.7.0\n",
            "Adding ptyprocess 0.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for certifi==2020.12.5\n",
            "Best match: certifi 2020.12.5\n",
            "Adding certifi 2020.12.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for wcwidth==0.2.5\n",
            "Best match: wcwidth 0.2.5\n",
            "Adding wcwidth 0.2.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==54.0.0\n",
            "Best match: setuptools 54.0.0\n",
            "Adding setuptools 54.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for joblib==1.0.1\n",
            "Best match: joblib 1.0.1\n",
            "Adding joblib 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for drqa==0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhQQezZpH92F"
      },
      "source": [
        "### Standford CoreNLP 설치\n",
        "\n",
        "> 영어 자연어처리에 유용한 툴을 제공하는 패키지\n",
        "\n",
        "> 참조: https://stanfordnlp.github.io/CoreNLP/\n",
        "\n",
        "<img src = \"https://stanfordnlp.github.io/CoreNLP/assets/images/pipeline.png\" width = \"600\" >\n",
        "\n",
        "#### **중요!!** 😎\n",
        "\n",
        "(1) 설치 시의 아래와 같은 항목이 나올 경우 `enter` 또는 `data/corenlp`를 입력\n",
        "\n",
        "```\n",
        "Specify download path or enter to use default (data/corenlp): '이 부분!'\n",
        "```\n",
        "\n",
        "(2) 설치 시의 아래와 같은 항목이 나올 경우 `yes`를 입력\n",
        "```\n",
        "/content/elasticsearch/DrQA Add to ~/.bashrc CLASSPATH (recommended)? [yes/no]: yes\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4Ab2-pdF5ZF",
        "outputId": "6cd764c9-7494-49a2-a32b-aaa111b35112"
      },
      "source": [
        "# CoreNLP 설치\n",
        "!./install_corenlp.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specify download path or enter to use default (data/corenlp): data/corenlp\n",
            "Will download to: data/corenlp\n",
            "/tmp /content/elasticsearch/DrQA\n",
            "--2021-03-03 05:21:52--  http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip [following]\n",
            "--2021-03-03 05:21:53--  https://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 390211140 (372M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-full-2017-06-09.zip’\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 372.13M  3.05MB/s    in 1m 41s  \n",
            "\n",
            "2021-03-03 05:23:35 (3.67 MB/s) - ‘stanford-corenlp-full-2017-06-09.zip’ saved [390211140/390211140]\n",
            "\n",
            "Archive:  stanford-corenlp-full-2017-06-09.zip\n",
            "   creating: stanford-corenlp-full-2017-06-09/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/README.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2017-06-09/sutime/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/sutime/english.holidays.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-javadoc.jar  \n",
            " extracting: stanford-corenlp-full-2017-06-09/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/build.xml  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/pom.xml  \n",
            "   creating: stanford-corenlp-full-2017-06-09/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2017-06-09/patterns/\n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2017-06-09/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2017-06-09/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2017-06-09/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/input.txt  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/Makefile  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2017-06-09/LICENSE.txt  \n",
            "/content/elasticsearch/DrQA\n",
            "Add to ~/.bashrc CLASSPATH (recommended)? [yes/no]: yes\n",
            "\n",
            "*** NOW RUN: ***\n",
            "\n",
            "export CLASSPATH=$CLASSPATH:data/corenlp/*\n",
            "\n",
            "****************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_8p2t8uKkev",
        "outputId": "ab2047eb-d62f-49c4-f349-36f821ec39d0"
      },
      "source": [
        "# python 3.6과 3.7에서의 충돌을 해결하기 위한 코드\n",
        "# 가볍게 실행하고 넘어가기\n",
        "!sudo apt-get remove python-pexpect python3-pexpect\n",
        "!sudo pip3.7 install --upgrade pexpect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'python3-pexpect' is not installed, so not removed\n",
            "Package 'python-pexpect' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Collecting pexpect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect) (0.7.0)\n",
            "\u001b[31mERROR: drqa 0.1.0 has requirement pexpect==4.2.1, but you'll have pexpect 4.8.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pexpect\n",
            "  Found existing installation: pexpect 4.2.1\n",
            "    Uninstalling pexpect-4.2.1:\n",
            "      Successfully uninstalled pexpect-4.2.1\n",
            "Successfully installed pexpect-4.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX6UtE8dE5Sv",
        "outputId": "e7652e4f-2971-4057-a72b-4ee853b74050"
      },
      "source": [
        "# WikiExtractor의 결과로 생성한 .json 파일을 db 파일로 변경하여 색인화 준비\n",
        "os.chdir('/content/elasticsearch/DrQA/scripts/retriever/')\n",
        "!python build_db.py '/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic/extract_result/AA/wiki_00' '/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic/sample_wiki.db'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/03/2021 05:30:32 AM: [ Reading into database... ]\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\n",
            "\r0it [00:00, ?it/s]\u001b[A\r1it [00:00, 28.38it/s]\n",
            "\r100% 1/1 [00:00<00:00, 27.69it/s]\n",
            "03/03/2021 05:30:32 AM: [ Read 6 docs. ]\n",
            "03/03/2021 05:30:32 AM: [ Committing... ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNxuJffaMZUn"
      },
      "source": [
        "os.chdir('/content/elasticsearch/DrQA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOh7g5PKtxuv",
        "outputId": "3f0a7e8f-9798-4fc2-d599-1e7309732019"
      },
      "source": [
        "import drqa.tokenizers\n",
        "import drqa.reader\n",
        "\n",
        "from drqa.retriever.doc_db import DocDB\n",
        "from elasticsearch import Elasticsearch\n",
        "from tqdm import tqdm\n",
        "from drqa.tokenizers import CoreNLPTokenizer\n",
        "from drqa.pipeline import DrQA\n",
        "from drqa.retriever import ElasticDocRanker, TfidfDocRanker\n",
        "\n",
        "# 새로운 인덱스 생성\n",
        "def make_index(es, index_name):\n",
        "    if es.indices.exists(index=index_name):\n",
        "        es.indices.delete(index=index_name)\n",
        "    print(es.indices.create(index=index_name))\n",
        "\n",
        "# 인덱스 명은 자유롭게\n",
        "index_name = 'test_index'\n",
        "make_index(es, index_name)\n",
        "\n",
        "# python build_db.py로 생성한 db의 경로\n",
        "db = DocDB(db_path='/content/gdrive/My Drive/Colab Notebooks/information_retrieval/elastic/sample_wiki.db')\n",
        "print(len(db.get_doc_ids())) # 6개\n",
        "\n",
        "# 색인화 진행\n",
        "for idx, doc_id in enumerate(tqdm(db.get_doc_ids())):\n",
        "    doc = {'title': doc_id, 'content': db.get_doc_text(doc_id)}\n",
        "    es.index(index = index_name, doc_type= 'string', body=doc)\n",
        "\n",
        "es.indices.refresh(index=index_name)\n",
        "\n",
        "# 색인된 엘라스틱서치 데이터베이스에서 'content'를 key로 하여, value에 질의를 입력하여 검색 (여기서는 film을 질의로 사용)\n",
        "results = es.search(index=index_name, body={'from':0, 'size':10, 'query':{'match':{'content':'film'}}})\n",
        "for result in results['hits']['hits']:\n",
        "    print('score:', result['_score'], 'source:', result['_source'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
            "  warnings.warn(message, category=ElasticsearchWarning)\n",
            "100%|██████████| 6/6 [00:00<00:00, 38.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'test_index'}\n",
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score: 1.8695081 source: {'title': '4', 'content': \"The Royal Cinema is an Art Moderne event venue and cinema in Toronto, Canada. It was built in 1939 and owned by Miss Ray Levinsky.\\nWhen it was built in 1939, it was called The Pylon, with an accompanying large sign at the front of the theatre. It included a roller-skating rink at the rear of the theatre, and a dance hall on the second floor.\\nIn the 1950's the theatre was purchased by Rocco Mastrangelo. In the 1990s, the theatre was renamed 'the Golden Princess'.\\nSince early 2007, Theatre D has owned and operated The Royal.\\nDuring the daytime it operates as a film and television post-production studio. It hosts film festivals including the European Union Film Festival and Japanese Movie Week.\\nThe Royal was featured in the 2013 film The F Word.\\nReferences.\\n \"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho0rQfbcTvYn"
      },
      "source": [
        "## 👍 실습 종료!! 정말 고생 많으셨습니다.\r\n",
        "\r\n",
        "## 🤥💦 모르는 것이 있을 경우에는? (Remind)\r\n",
        "\r\n",
        "프로그래밍의 첫 걸음은 `검색` 입니다.\r\n",
        "\r\n",
        "특히 `빅데이터와 정보 검색` 수업을 듣고 있는 여러분들은 반.드.시 궁금하거나 모르는 내용에 대해서 `검색`하는 습관이 필요합니다.\r\n",
        "\r\n",
        "> Google 검색 엔진 \r\n",
        "                \r\n",
        "    -> \"Python 루프문\"\r\n",
        "                 \r\n",
        "    -> \"Python for문 에러\"\r\n",
        "\r\n",
        "    -> \"What is SyntaxError: invalid syntax in python?\"\r\n",
        "\r\n",
        "등의 `에러문장`과 `키워드`를 적극 활용해주세요!😄\r\n",
        "\r\n"
      ]
    }
  ]
}